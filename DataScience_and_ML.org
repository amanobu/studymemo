- 11
  - 確率密度関数
    - あるデータが特定の範囲に収まる確率
    - 連続的
  - 確率質量関数
    - 確率密度関数の離散的値を取る
- 12
  - 正規分布
    - import matplotlib.pyplot as plt
    - from scipy.stats import norm
    - x = xp.arange(-3,3.0.01)
    - plt.plot(x,norm.pdf(x))
  - 正規分布に沿って乱数を発生
    - import numpy as np
    - import matplotlib.pyplot as plt
    - mu = 5.0
    - sigma = 2.0
    - values = np.random.normal(mu,sigma,10000)
    - plt.hist(values,50)
    - plt.show()
  - 指数確率密度関数
    - expのような確率密度関数
    - from scipy.stats import expon
    - import matplotlib.pyplot as plt
    - x = np.arange(0,10,0.001)
    - plt.plot(x,expon.pdf(x))
  - 2項確率質量関数
    - from scipy.stats import binom
    - import matplotlib.pyplot as plt
    - n,p = 10,0.5
    - x = np.arange(0,10,0.001)
    - plt.plot(x.binom.pmf(x,n,p))
    - 離散的なやつ
  - ポアソン確率質量関数
    - 正規分布とは違う(何が違うんだ？）
    - ウェブサイトの訪問者数が一日平均500人。550人となる確率は？
    - from scipy.stats import poisson
    - mu = 500
    - x = np.arange(400,600,0.5)
    - plt.plot(x,poisson.pmf(x,mu))
    - グラフを描いてみればわかるが,0.2%ぐらい
- 13 パーセンタイルとモーメント
  - パーセンタイル
    - データーセットにおいてX%の値がその値より下の点
    - vals =np.random.normal(0,0.5,10000)
    - np.percentile(vals,50)
      - この値より下の値が全体の50%を占めるという意味の値となる
  - モーメント
    - 確率密度関数の形状の定量化
    - 1次：平均
    - 2次：分散
    - 3次：歪度
      - 分布がどれだけ偏っているか
      - 右側に偏っているのであれば、負の値
      - 左側なら正の値
    - 4次：尖度
      - ピークがどれだけとがっているか
        - ピークが高ければこの値は大きい
    - この1次から4次の値を使うことで、確率密度関数の形状を言い表せる
    - vals = np.random.normal(0,0.5,10000)
    - 1:np.mean(vals)
    - 2:np.var(vals)
    - 3:
      - import scipy.stats as sp
      - sp.skew(vals)
    - 4:sp.kurtosis(vals)
- 14:matplotlib
  - from scipy.stats import norm
  - import matplotlib.pyplot as plt
  - import numpy as np
  - x = np.arange(-3,3,0.001)
  - 
  - 軸の調整
    - axes = plt.axes()
    - 上のオブジェクトに対していろいろ設定を行っていく
    - axes.xlim,ylim x軸、y軸の範囲を設定
    - axes.set_xtics,set_yticsでメモリの付与
  - グリッドの追加
    - axes.grid()
  - 線の種類
    - plt.plot(x.norm.pdf(x),'b-')
      - b:blue
      - -:solid
    - :点線
    - r:赤
  - 軸のラベルと凡例
    - plt.xlabel('X軸の意味')
    - plt.legent(['1番目のグラフの意味',...])
  - XKCD
    - 漫画調のグラフ
    - plt.xkcd()
      - だけでほぼOK
  - 円グラフ
    - values = [12,55,4,32,14]
    - colors = ['r','g','b','c','m']
    - explode = [0,0,0.2,0,0] #隙間
    - labels = ['....'.....]
    - plt.pic(values,colors=colors,label=labels,explod=explode)
  - 棒グラフ
    - plt.bar(範囲,値,色)
  - 散布図
    - plt.scatter(randn(500),randn(500))
  - ヒストグラム
    - plt.hist(...)
  - 箱ひげ図
    - 使わなそうだから後で
- 15:共分散と相関
  - 共分散
    - 2組のデータセットの関係を表す数値
    - ０に近い場合関係が浅い
    - 正でも不でも大きい値であれば、関係が高い
    - 計算
      - 同じ長さのデータセットを用意
      - それぞれの平均を引く
      - それぞれのデータセットを高次元ベクトルとし、内積を計算
      - それをサンプルの長さで割る
  - 相関の計算
    - 共分散を理解するのはむずかいしい
    - 共分散をそれぞれのデータセットの標準偏差で割り、正規化
    - -1なら逆相関
    - 0：相関なし
    - 1：完全な相関がある
- 42:K近傍法(KNN)
  - 新しい点が来たら、その近くにあるものだどちらかによって判定する
- 47:強化学習
  - 進捗情報（変化値）を学習しその後の判断の材料とすること？
  - Q学習
    - 強化学習の一つ
      - 条件
        - 環境における状態s
        - とることが可能な行動a
        - 各状態と行動に紐づいた値Q
      - Qの値が0
      - 空間を探索
      - 悪い状態・行動が起こるたびにQを現象
      - 逆ならばQを増加
      - 次のQの計算
        - Q(s,a) += discount*(reward(s,a)+Max(Q(s'))-Q(s,a)
          - s':現在の状態
          - s:前の状態
          - なんとなくわかるような気がするけど、rewardとかdiscountとかなに？
    - すべてのパターンの探索
      - 全パターンの探索
      - ある一定の閾値以下になったものを採用(εと言っていた)
  - 強化学習の参考
    - URLが資料に記載されているので参考
    - パックマンのサンプルもある模様
- 54:Apache Spark
  - SparkContext
    - 設定をしてインスタンス化するとそのSparkの実行環境のようなオブジェクトだと思う
  - RDD：重要なオブジェクト
    - 並列な実行可能なデータセットの定義
    - これをデータを表現して並列処理するらしい
    - いろんなデータソースからインスタンス化する
      - JDBC,CSV,JSON,Elastisearch...
    - map,filter,union....などの操作をする
      - イメージ
        - rdd = sc.parallelize([1,2,3,4])
        - rdd.map(lambda x:x*x)
    - あたいの返却
      - collect,count,top,reduce...
    - 遅延評価
  - MLlib：まーいろいろ
    - 特徴抽出：単語出現頻度
    - 基礎的な統計：かい2乗検定・平均・分散
    - 線形、ロジスティック回帰
    - SVN
    - 主成分分析、特異値分解
- 59/60:Apache Sparkの 決定木のコード・KMeanSのコード
  - 資料にサンプルコードあり
    - データをロードして、正規化
    - RDDをつくり
    - trainして
    - 値を取り出す
    - ほとんどscikit-learnとやる順番は変わらない
  - spark-submit コード　で実行
- 61:TF-IDF
  - TF:Term Frequency 出現頻度
    - 単語の必現頻度
  - Inverse Document Frequency 逆文章頻度
    - Document Frequency
      - 存在するすべての文章の出現頻度
  - TF/DF :その単語がその文章にとってどれだけ重要・特徴を表しているかの指標
    - TF * IDFも同じ指標となる
  - 効率化の為単語は数値で表現されrう
  - 例
    - 検索アルゴリズム
      - 集めたすべての単語でTF-IDFを計算
      - ある検索語でTF-IDFのスコアすべての文章をスコア順に表示
- 62:WikiPediaへの適用
  - 簡単な検索エンジンが簡単に作れる
  - コードは資料に
  - 単語はハッシュ値で格納されるので、検索語もハッシュ値に変換後検索される
- 63:Sparkでの線形回帰
  - Spark2.0～使えるDataFrame
  - 線形回帰ができるようになった
- 64:A/Bテスト
  - WEBサイトの変更がどのような影響を与えるのかを測定する
  - テスト内容
    - デザイン：前後のデザインが申し込みされ宇rの科
    - UIフロー：申し込みが多いフローはどちらか
    - アルゴリズム：アルゴリズムの差でどれぐらい影響を与えるのか
    - 価格：どの価格が買ってもらえるか
    - 名称：どの名称が良いか
  - よくある間違い：ばらつき
    - テストを短い期間しか行わなかったので分析できるログが少ない
      - 購入量のばらつきがあったが、それを考慮せず変更した
    - 時間をかけて検証する必要がある
- 65:t検定とp値
  - A/Bテストの結果偶然なのか、意味あるものなのかを判定する
  - t検定
    - 2つのデータセットの各値の差の標準偏差および差の分散から統計量tを計算
    - tの値が大きければ2つのデータセットに差異があるということになる
    - 正規化分布が前提
    - フィッシャーの性格確率検定：クリックスルー率
    - e検定：ユーザ一人当たりの取引
    - かい2乗検定：商品の購入量
  - p値
    - AとBが帰無仮説を満たしている確率
      - 帰無仮説：AとBの分布が全くのランダムで二つの有意差がない
    - p値が低いと有意である
    - 帰無仮説を仮定して極端なt値を取る確率
    - A/Bでランダムではありえない分布をしている場合はどちらかに有意な状態になっているだろうと判定
    - p値の使用
      - 有意水準を決定する
      - 実験終了後p値の測定を行い、有意水準より小さければ帰無仮説を棄却
        - つまりA/Bで有意な差があると思われる
          - 断定はできないが偶然とは考えにくい（偶然かもしれない可能性はある）
- 66:実際にt検定とp値の計算
  - scipyのstatsのttest_indを使う
    - A = np.random.normal(25,5.0,10000)
    - B = np.random.normal(26,5.0,10000) #Bを少しずらした
    - stats.ttest_ind(A,B)
      - statistic,pvalueの値が返却される
- 67:A/Bテストの実験をどこまで続けるべきか
  - p値が有意水準1,5%になった場合はよかったのでそれをもとに修正する
  - p値が時間が経過しても値が小さくならず、発散したり、分散すると、収束しない
  - 制限時間を決めて臨むべき
  - 1回の実験で実施するのは通常1つ（複数やらないのですね）
- 68:A/Bテストの落とし穴
  - 相関は因果関係を意味するものではない
  - 偶然の可能性orほかの要因も考えられる
  - Webサイトの変更はなじんでいるユーザの関心を引きやすい
    - 新しくなったからという理由だけでクリックされているかもしれない
    - しばらく経ってから再評価するべき
  - 季節的なもの
    - クリスマス時期の消費行動
  - データの分散
    - 正しく分散されているか。偏ってないか
    - チェックとのためにA/Aテストをやるほうが良いらしい
  - データ汚染
    - ロボットアクセスなど、比較対象と関係ないデータは除去するべき
  - 帰属の誤り
    - 行動の変化がどの実験にかかわっているかを誤る場合がある
      - 自作のより一般的に使われているA/Bテストのプラットフォームを利用すればこの誤りから回避できるらしい
        - ここの意味がよく分からない
    - 他の変更が影響していないか？
    - 複数の実験を実施していないか？
- 69:更に学ぶために
  - おすすめ
    - Data Science From Scratch
    - KDNuggets.com
      - 老舗
      - 様々なデータセットを得られる

