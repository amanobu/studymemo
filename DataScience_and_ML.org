- 6:平均値・中央値・モード
  - モード：データセットにおけるもっとも頻度の高い値
- 10：分散、標準偏差
  - 分散(σ^2)
    - データがどれだけ広がっているか
    - 分散は各データと平均値との差の2乗平均
  - 標準偏差(σ)
    - 分散の平方根
    - 外れ値の検出。平均値から標準偏差１つ以上離れたデータは通常データではないと考える
    - 極端な値かどうかは平均値から何σ分離れているかで判断
  - ただし、すべてのデータセット(母集団)を使うのではなく、標本データを対象としている場合は、母分散ではなく不偏標本分散を使うべきらしい
    - 母集団すべての解析をできればよいが、通常は時間とコストが無理なので標本データを使う
      - そこから、平均・分散・標準偏差を推定する
    - Nこのサンプルならば、Nで割るのではなく、N-1で割る
    - 一般的に標本データの分散は母分散より小さくなる傾向らしい
    - 小さくなる効果を打ち消し、母集団の値に近づけている
- 11
  - 正規分布では、標準偏差σ分によってその範囲に収まる確率が決まっている
    - +1σで34.1%,+2σで13.6%,+3σで2.1%
    - ±3σまでで、99.6%
  - 確率密度関数
    - あるデータが特定の範囲に収まる確率
    - 連続的
  - 確率質量関数
    - 確率密度関数の離散的値を取る
  - 範囲外
    - 二項分布
      - ベルヌーイ試行：試行結果が2種類しかないもの
        - 〇か×か、成功か失敗か、表か裏か
      - お互いに独立したベルヌーイ試行をの分布を二項分布という
        - サイコロを２００回投げた時に１の目がk回でる確率の分布
      - 2項分布はばらつきが十分に大きければ正規分布に近似できる
    - ポアソン分布
      - 単位時間当たり平均λ回起こる事象が単位時間にk回起きる確率を会わらすのに使われる確率分布をポアソン分布
      - https://ja.wikipedia.org/wiki/%E3%83%9D%E3%82%A2%E3%82%BD%E3%83%B3%E5%88%86%E5%B8%83#/media/File:Poisson_pmf.svg
        - こんな感じの波が進む雰囲気のグラフ
      - ランダムに起きる事象について特定期間中に起きる確率が何％みたいなことに使われる
        - 30分に平均2回電話がかかって来るコールセンターにおいて、1時間に6回電話がかかって来る確率
        - 1年あたり平均0.61人の兵士が馬に蹴られて死ぬ軍隊において、「1年に何人の兵士が馬に蹴られて死ぬかの確率」
        - サーバのアクセス数
      - 「完全にランダムではない事象」に対しては正確な分析が出来ない
        - 他の事象と相関があるケースではポアソン分布は使えないとのこと
- 12
  - 正規分布
    - import matplotlib.pyplot as plt
    - from scipy.stats import norm
    - x = xp.arange(-3,3.0.01)
    - plt.plot(x,norm.pdf(x))
  - 正規分布に沿って乱数を発生
    - import numpy as np
    - import matplotlib.pyplot as plt
    - mu = 5.0
    - sigma = 2.0
    - values = np.random.normal(mu,sigma,10000)
    - plt.hist(values,50)
    - plt.show()
  - 指数確率密度関数
    - expのような確率密度関数
    - from scipy.stats import expon
    - import matplotlib.pyplot as plt
    - x = np.arange(0,10,0.001)
    - plt.plot(x,expon.pdf(x))
  - 2項確率質量関数
    - from scipy.stats import binom
    - import matplotlib.pyplot as plt
    - n,p = 10,0.5
    - x = np.arange(0,10,0.001)
    - plt.plot(x.binom.pmf(x,n,p))
    - 離散的なやつ
  - ポアソン確率質量関数
    - 正規分布とは違う(何が違うんだ？）
    - ウェブサイトの訪問者数が一日平均500人。550人となる確率は？
    - from scipy.stats import poisson
    - mu = 500
    - x = np.arange(400,600,0.5)
    - plt.plot(x,poisson.pmf(x,mu))
    - グラフを描いてみればわかるが,0.2%ぐらい
- 13 パーセンタイルとモーメント
  - パーセンタイル
    - データーセットにおいてX%の値がその値より下の点
    - vals =np.random.normal(0,0.5,10000)
    - np.percentile(vals,50)
      - この値より下の値が全体の50%を占めるという意味の値となる
  - モーメント
    - 確率密度関数の形状の定量化
    - 1次：平均
    - 2次：分散
    - 3次：歪度
      - 分布がどれだけ偏っているか
      - 右側に偏っているのであれば、負の値
      - 左側なら正の値
    - 4次：尖度
      - ピークがどれだけとがっているか
        - ピークが高ければこの値は大きい
    - この1次から4次の値を使うことで、確率密度関数の形状を言い表せる
    - vals = np.random.normal(0,0.5,10000)
    - 1:np.mean(vals)
    - 2:np.var(vals)
    - 3:
      - import scipy.stats as sp
      - sp.skew(vals)
    - 4:sp.kurtosis(vals)
- 14:matplotlib
  - from scipy.stats import norm
  - import matplotlib.pyplot as plt
  - import numpy as np
  - x = np.arange(-3,3,0.001)
  - 
  - 軸の調整
    - axes = plt.axes()
    - 上のオブジェクトに対していろいろ設定を行っていく
    - axes.xlim,ylim x軸、y軸の範囲を設定
    - axes.set_xtics,set_yticsでメモリの付与
  - グリッドの追加
    - axes.grid()
  - 線の種類
    - plt.plot(x.norm.pdf(x),'b-')
      - b:blue
      - -:solid
    - :点線
    - r:赤
  - 軸のラベルと凡例
    - plt.xlabel('X軸の意味')
    - plt.legent(['1番目のグラフの意味',...])
  - XKCD
    - 漫画調のグラフ
    - plt.xkcd()
      - だけでほぼOK
  - 円グラフ
    - values = [12,55,4,32,14]
    - colors = ['r','g','b','c','m']
    - explode = [0,0,0.2,0,0] #隙間
    - labels = ['....'.....]
    - plt.pic(values,colors=colors,label=labels,explod=explode)
  - 棒グラフ
    - plt.bar(範囲,値,色)
  - 散布図
    - plt.scatter(randn(500),randn(500))
  - ヒストグラム
    - plt.hist(...)
  - 箱ひげ図
    - 使わなそうだから後で
- 15:共分散と相関
  - 共分散
    - 2組のデータセットの関係を表す数値
    - ０に近い場合関係が浅い
    - 正でも不でも大きい値であれば、関係あることを意味する
    - 計算方法
      - 同じ長さのデータセットを用意
      - それぞれの平均を引く
      - それぞれのデータセットを高次元ベクトルとし、内積を計算
      - それをサンプルの長さで割る
    - numpyのcov関数で共分散を計算できる
  - 相関の計算
    - 共分散を理解するのはむずかいしい。なので相関を通常は使う
    - 共分散をそれぞれのデータセットの標準偏差で割り、正規化
    - -1なら逆相関
    - 0：相関なし
    - 1：完全な相関がある
    - 相関は因果関係を表してはいない！
    - numpyのcorrcoefメソッドで相関を計算できる
- 16:条件付き確率
  - P(A,B):お互いに依存する2つのイベント両方が起きる確率
  - P(B|A):Aが起きた場合にBが起きる確率
    - P(B|A)=P(A,B) / P(A)で表せる
- 17:ベイズの定理
  - P(A|B) = P(A)P(B|A) / P(B)
  - Bが前提でAが起きる確率はAの確率にAが前提でBが起きる確率をかけて、Bの確率で割った値に等しい
- 29:エントロピー
  - データセットの乱雑さの尺度
    - 各データがどれだけ同じか異なるか
  - エントロピーがが低いと同じ
  - 異なるとエントロピーは高くなる
- 30:決定木
  - 決定ルールをもとにした木構造のフローチャート
  - 教師あり学習の一つ
  - 例
    - 過去の採用データから履歴書をフィルタリングする
      - 決定ルール：最終学歴とか、インターンか、年齢とか
  - 各ステップでエントロピーを最小にできるようなデータセットを分割できるような属性を見つける
    - がっつり絞り込みをできるかどうかということだと思う
  - ランダムフォレスト
    - 決定木は過剰適合しやすい
    - そのため複数の決定木を用意して最終結果を投票する
- 33:アンサンブル学習
  - 同じ問題に対して複数のモデルを適用し、投票によって決定する
  - ランダムに取り出されたデータをもとに複数のモデルを訓練する
- 42:K近傍法(KNN)
  - 新しい点が来たら、その近くにあるものだどちらかによって判定する
- 47:強化学習
  - 進捗情報（変化値）を学習しその後の判断の材料とすること？
  - Q学習
    - 強化学習の一つ
      - 条件
        - 環境における状態s
        - とることが可能な行動a
        - 各状態と行動に紐づいた値Q
      - Qの値が0
      - 空間を探索
      - 悪い状態・行動が起こるたびにQを現象
      - 逆ならばQを増加
      - 次のQの計算
        - Q(s,a) += discount*(reward(s,a)+Max(Q(s'))-Q(s,a)
          - s':現在の状態
          - s:前の状態
          - なんとなくわかるような気がするけど、rewardとかdiscountとかなに？
    - すべてのパターンの探索
      - 全パターンの探索
      - ある一定の閾値以下になったものを採用(εと言っていた)
  - 強化学習の参考
    - URLが資料に記載されているので参考
    - パックマンのサンプルもある模様
- 50:データクリーニング
  - データサイエンティストの多くの時間はデータの収集とクリーニングに費やされる
  - データをよく見て検証
  - 結果に疑問を持つこと
- 51:データクリーニングの実践
  - アクセスログからもっとも見られたページを解析してみる
  - あとでやってみる
- 53:外れ値の話
  - なぜ外れ値があるのか
  - それを除去していいのか？というのを調べる必要がある
    - 結果にその外れ値を含むべきなのかどうか
  - 除去
    - u = np.median(data) #平均値
    - s = np.std(data) #標準偏差
    - filtered = [e for e in data if (u - 2 * s < e < u + 2 * s) ]
      - dataより標準偏差二つ分以上離れたデータを除去する
- 54:Apache Spark
  - SparkContext
    - 設定をしてインスタンス化するとそのSparkの実行環境のようなオブジェクトだと思う
  - RDD：重要なオブジェクト
    - 並列な実行可能なデータセットの定義
    - これをデータを表現して並列処理するらしい
    - いろんなデータソースからインスタンス化する
      - JDBC,CSV,JSON,Elastisearch...
    - map,filter,union....などの操作をする
      - イメージ
        - rdd = sc.parallelize([1,2,3,4])
        - rdd.map(lambda x:x*x)
    - あたいの返却
      - collect,count,top,reduce...
    - 遅延評価
  - MLlib：まーいろいろ
    - 特徴抽出：単語出現頻度
    - 基礎的な統計：かい2乗検定・平均・分散
    - 線形、ロジスティック回帰
    - SVN
    - 主成分分析、特異値分解
- 59/60:Apache Sparkの 決定木のコード・KMeanSのコード
  - 資料にサンプルコードあり
    - データをロードして、正規化
    - RDDをつくり
    - trainして
    - 値を取り出す
    - ほとんどscikit-learnとやる順番は変わらない
  - spark-submit コード　で実行
- 61:TF-IDF
  - TF:Term Frequency 出現頻度
    - 単語の必現頻度
  - Inverse Document Frequency 逆文章頻度
    - Document Frequency
      - 存在するすべての文章の出現頻度
  - TF/DF :その単語がその文章にとってどれだけ重要・特徴を表しているかの指標
    - TF * IDFも同じ指標となる
  - 効率化の為単語は数値で表現されrう
  - 例
    - 検索アルゴリズム
      - 集めたすべての単語でTF-IDFを計算
      - ある検索語でTF-IDFのスコアすべての文章をスコア順に表示
- 62:WikiPediaへの適用
  - 簡単な検索エンジンが簡単に作れる
  - コードは資料に
  - 単語はハッシュ値で格納されるので、検索語もハッシュ値に変換後検索される
- 63:Sparkでの線形回帰
  - Spark2.0～使えるDataFrame
  - 線形回帰ができるようになった
- 64:A/Bテスト
  - WEBサイトの変更がどのような影響を与えるのかを測定する
  - テスト内容
    - デザイン：前後のデザインが申し込みされ宇rの科
    - UIフロー：申し込みが多いフローはどちらか
    - アルゴリズム：アルゴリズムの差でどれぐらい影響を与えるのか
    - 価格：どの価格が買ってもらえるか
    - 名称：どの名称が良いか
  - よくある間違い：ばらつき
    - テストを短い期間しか行わなかったので分析できるログが少ない
      - 購入量のばらつきがあったが、それを考慮せず変更した
    - 時間をかけて検証する必要がある
- 65:t検定とp値
  - A/Bテストの結果偶然なのか、意味あるものなのかを判定する
  - t検定
    - 2つのデータセットの各値の差の標準偏差および差の分散から統計量tを計算
    - tの値が大きければ2つのデータセットに差異があるということになる
    - 正規化分布が前提
    - フィッシャーの性格確率検定：クリックスルー率
    - e検定：ユーザ一人当たりの取引
    - かい2乗検定：商品の購入量
  - p値
    - AとBが帰無仮説を満たしている確率
      - 帰無仮説：AとBの分布が全くのランダムで二つの有意差がない
    - p値が低いと有意である
    - 帰無仮説を仮定して極端なt値を取る確率
    - A/Bでランダムではありえない分布をしている場合はどちらかに有意な状態になっているだろうと判定
    - p値の使用
      - 有意水準を決定する
      - 実験終了後p値の測定を行い、有意水準より小さければ帰無仮説を棄却
        - つまりA/Bで有意な差があると思われる
          - 断定はできないが偶然とは考えにくい（偶然かもしれない可能性はある）
- 66:実際にt検定とp値の計算
  - scipyのstatsのttest_indを使う
    - A = np.random.normal(25,5.0,10000)
    - B = np.random.normal(26,5.0,10000) #Bを少しずらした
    - stats.ttest_ind(A,B)
      - statistic,pvalueの値が返却される
- 67:A/Bテストの実験をどこまで続けるべきか
  - p値が有意水準1,5%になった場合はよかったのでそれをもとに修正する
  - p値が時間が経過しても値が小さくならず、発散したり、分散すると、収束しない
  - 制限時間を決めて臨むべき
  - 1回の実験で実施するのは通常1つ（複数やらないのですね）
- 68:A/Bテストの落とし穴
  - 相関は因果関係を意味するものではない
  - 偶然の可能性orほかの要因も考えられる
  - Webサイトの変更はなじんでいるユーザの関心を引きやすい
    - 新しくなったからという理由だけでクリックされているかもしれない
    - しばらく経ってから再評価するべき
  - 季節的なもの
    - クリスマス時期の消費行動
  - データの分散
    - 正しく分散されているか。偏ってないか
    - チェックとのためにA/Aテストをやるほうが良いらしい
  - データ汚染
    - ロボットアクセスなど、比較対象と関係ないデータは除去するべき
  - 帰属の誤り
    - 行動の変化がどの実験にかかわっているかを誤る場合がある
      - 自作のより一般的に使われているA/Bテストのプラットフォームを利用すればこの誤りから回避できるらしい
        - ここの意味がよく分からない
    - 他の変更が影響していないか？
    - 複数の実験を実施していないか？
- 69:更に学ぶために
  - おすすめ
    - Data Science From Scratch
    - KDNuggets.com
      - 老舗
      - 様々なデータセットを得られる

>>>>>>> d0da91b5f4d07eb240e7b9f44b055784865befcf
