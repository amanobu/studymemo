* 機械学習のアルゴリズムでどれを使うべきかのすばらしい資料
http://scikit-learn.org/stable/tutorial/machine_learning_map/


* Courseraの機械学習コースのメモ
目的関数：フィッティングさせたい関数。未知の値に対して判定する関数
コスト関数：よくJ(θ)と記載→これのぱらめーたθを最小化する問題に帰着する

パラメータθが多い多項式で回答に近似するθを最小化する問題:線形回帰
θ1x1+θ2x2+θ3x3......
大体の最小2乗法で解く
最急降下法　Gradient Descent
・パラメータがたくさんあってもOK
・時間がかかる、繰り返しが多い

正規方程式 Normal Equation：微分値=0として、直接最適値を解いてしまう
・パラメータがたくさんあると遅い
・シンプル、繰り返さない
大体θが1マンぐらいなら正規方程式かなぁ

クラス分け問題は線形回帰は適用するのは無理がある
→ロジスティク関数・シグモイド関数
でも結局最急降下法に落ち着く
------
Feature scaling
θの範囲が同じような範囲にないと等高線が激しく歪み、ステップが多くなる
そのため-1から1の近い値になるようにする
X0=1だから：切片項

Mean normalization
ほかのやり方
X1 <- (X1-Xの平均)/(MAX-MIN)



------

ロジスティック関数
g(z) >= 0.5ならばy=1 <0.5ならばy=0
つまり
Θ(T)*x>=0ならばy=1 <0ならばy=0

------
高次の多項式
すべてのパラメータに対してθを求めると、オーバーふぃってんぐする
なので不要なパラメータを打ち消しするためにはそのパラメータのθの値を大きくとれば打ち消し出来る
→線形回帰でも、ロジスティック関数でも、コスト関数の後に項を追加する


ニューラルネットワークのコスト関数はロジスティック関数とほぼ同じ

ニューラルネットワークのコスト関数のチェックのため（正しく動いているか試すため）、
ある値+ε,ある値-εの傾きを調べ、コスト関数と大体あっているか調べる
→それにより、実装の内容がほぼ正しいと判る

ランダム初期化：ニューラルネットワークのパラメータの初期化
θ：ウェイトは同じにしては結局値が一致してしまう

------

正しい実装のチェックの為、テストデータを
訓練用データ50%
次元数決定用データ25%
クロスバリデーションセット25%
に分割。
訓練用データででθを決めた後、次元数決定用データでどの次元が良いか分析する(オーバーふぃってんぐせず、うんだーふぃってんぐしないような物が選択出来ているか）
Θを決めるには、x2乗～10乗とかでいろいろ試してみて、CVセットの成績でどのΘを再送するか決める
その後動作確認用データでアルゴリズムもあっているかどうかを確認

何かパラメータを決めるとき数パラメータから実行する
→学習曲線（どれだけあっているか）が表現できて、指標がわかる

間違えて判定をしてしまった物を見つけ、それを解析し、futureの再検討するのがよい

または、その物を数値化できるようにしたら、判断がわかりやすい

bias：偏り
variance:分散
二つはトレードオフ

high bias :アンダーフィットしがち
high variance:オーバーフィットしがち

アンダーフィットの改善
もっとトレーニングデータを
futureの縮小化を
λを大きい値に

オーバーフィットの改善
もっとfutureの追加
多項式の追加
λを小さい値に


単一評価(クロスバリデーションセットで正しく判定出来た割合など)ではなく
再現率
精度
という指標はどうか？
→
| \ | 1              | 0              |
| 1 | 1と予想して1① | 1と予想して0② |
| 0 | 0と予想して1③ | 0と予想して0④ |

精度p:①/①+②
再現率r:①/①+③
F1=(2pr)/(r+p)
F1値：アルゴリズムの指標の一つ

ロジスティック関数の閾値を0.5で評価しているが、その閾値を上下させることで精度・再現率が変化する

無闇にデータを増やすのではなく、精度、再現率かを見ることは重要だが、ただデータを大量に集めることでよくなる条件がある
フューチャーから推測して、それがよい判断条件になるか推測する（関係無いフューチャーならば推測は難しいだろうという判断）
英語の穴あきで、前後の文の情報→当てはまる。→大量のデータで学習するのは良い
家のサイズから、この価格を推測→難しい→大量にデータを集めても難しい
複雑なネットワーク・パラメータがある場合に本当に大量のデータがあると、優れたアルゴリズムよりよく働く

SVM
ロジスティック関数を変形した目的関数
決定境界をいい感じに引いてくれる
ある程度のバッファ(マージン)を保つようになる

SVM+Nonカーネル：線形分離
SVM+ガルシアンカーネル：複雑な曲線分離可能：非線形

カーネル
線形で分離できない時のような場合、通常であれば多項式多次元の目的関数を解き、パラメータΘにフィットさせるが
計算量的に高くつく。
それをもっと押さえる方法
新たにフィーチャーを定義する
f1=(||x-l||^2)/σ^2　つまりランドマークlとxの距離(類似度)をσ^2で割る
x,yが与えられたときにランドマークとの類似度をはかる関数にかける
近いときは、1に近い値で、遠い時は0にちかい値になる
→ガウスカーネルと呼ぶ

SVMをつかううえで
・パラメータＣを決める
・KERNELに何を使うか決める
　・線形か：線形で分類できそうな物
　・ガルシアンカーネルのような非線形となりそうな物か

・定数Ｃ
大きくする Large C: Lower bias, high variance. 
小さくする Small C: Higher bias, low variance. アンダーフィットしがち
・σ^2
大きくする Higher bias, lower variance. 
小さくする Lower bias, higher variance. 




教師なし学習
K-means
よく使われるクラスタリングアルゴリズム

パラメータの次元を減らす
PCA解析


リコメンドシステム
まずはコンテントベースの
| movie  | user1 | user2 | user3 | x1 |  x2 |        |
| movie1 |     5 |     5 |     0 |  1 | 0.9 | ←x(1) |
| movie2 |     5 |     ? |     ? |    |     |        |
| movie3 |     ? |     4 |     0 |    |     |        |
x1は何かしらの評価（action,loveだったり）

x(1)が何らかの事で分かっている
またUser1のパラメータΘも分かっているとすると

user1のmovie3の評価はΘT*X(1)でだせる


大規模データの取り扱い
本当に大量のテストデータが取得できる場合、XからYを導き出せそうかイメージすることが大事
たくさんのパラメータがあってもいい感じにフィットするようになる
　－＞高BIASにもならずoverfitもせず

偏りが激しい場合
９９％対１％のクラス分けの場合、実装した分類機が良い結果なのかの判定
精度と再現率は相反するもの
相反するアルゴリズムを判定するのにF1スコアというものがある
2(PR)/P+R
P:精度
R:再現率
指標はいろいろあるが、機械学習で使っている指標でメジャーみたいだ

ロジスティック関数の閾値を０．５としていたが、閾値を上げると精度は上がるが再現率は低くなる
逆に閾値を下げると、精度は下がり、再現率は高くなる

最急降下法はパラメータΘを1回修正するたびに全データの和をとるアルゴリズム。大量にデータがあった場合非常に時間がかかる。（バッチ最急降下法）

確率的勾配降下法→全和をとるのではなく、１トレーニングデータでΘを更新しちゃう。
→よって早い。が、トレーニングパラメータαによって発散したり、最適解に落ちずぐるぐる回ったりする。

バッチ最急降下法をちょっと確率的勾配降下法にしたもの
ミニバッチ最急降下法：確率的勾配降下法は１ステップだが、ある程度まとまった範囲で和をとり、Θを更新する
→ミニバッチにしても確率的勾配降下法にしても、1回Θが更新されるたびに学習率をグラフにしてみて、緩やかにでも下降している事を確認した方がよい

------

アノマリー検出
nomalじゃないことの検出
ガウス分布：正規分布

μ：山の中心：すべての平均
σの2乗：山の広がり：分散

Π：積の記号（Σは和の記号）

パラメータxを集め
μとσ2(sigmaの2状）を計算する

Πp(x(1),u(1),σ2(1))

山の中心に近いものであれば、それは通常。それ以外、山の下腹にあるものは異常だろうと判断する

１．サンプルの６割を正常のテストセット
２．2割をCVセット（これはエラーを含む）
３．2割をTESTセット（これはエラーを含む）
とする
１でμとσ2を計算モデルを作る
CVやTESTセットを使いモデルがイケてるかを判定する
※ま、アノマリー検出ということで無く一般的な事や

そんなに異常なテストサンプルが無いときは（沢山の正常系のサンプルがある）アノマリー検出
同数の場合は、教師なり学習を使うべきだ




* 機械学習のステップ
自分自身のまとめ
- 説きたい問題の対象を理解する
  - 教師あり？
  - 教師なし？
  - データがどれぐらい手に入る？
    - 自動生成できる？
  - フィーチャーは？
  - 線形分離？複雑？
  - 正規分布している？
- 教師無し
  - クラスタリング
  - 主成分分析
  - こっちは特にまとめは不要かな
- 教師あり
  - 解きたい問題から良さそうなアルゴリズムを決定する
    - ほとんどどれを使うにしても似通った結果になる
      - ただし計算量とかは違う
  - トレーニングセット、CVセット、テストセットに分割
  - トレーニングセットにて学習
  - 数値化＆学習曲線のプロットし、構築したモデルの分析
    - このままＧＯ？
    - だめ
      - HIGH BIAS?
      - HIGH VARIANCE?
      - フューチャーの選択
        - 複数次元のモデルを再計算させてみて、一番良い結果の物を採用
      - 正規化項の検討
        - λ(ロジスティック/線形)やC(SVM)
      - 改善するべき層の確認
        - 例のPHOTO OCRの多重層の様に


* Deep lerning
  - http://nnadl-ja.github.io/nnadl_site_ja/index.html 
  - ニューラルネットワーク
    - パーセプトロンの集合
  - パーセプトロン
    - 0,or 1をinputにとり、1,or 0を出力
    - 出力の決定は、入力の重み付き総和としきい値できまる
  - 入力：x
  - 重み：w
    - 入力が出力に及ぼす影響の大きさを表す実数
  - バイアス：b
    - しきい値のこと
    - 1を出力する傾向の高さ
  - 学習すると言うこと
    - 重みとバイアスを自動的に最適化しする事で正しい答えを出すニューラルネットワークを維持する事
    - パーセプトロンだとこの学習は起こらない
      - なぜならば、0か1なので反転する可能性がある。微妙なさじ加減ができない
  - シグモイドニューロン・ロジスティック関数：σ
    - パーセプトロンとは違い0から1の間の出力をする
    - グラフの形が重要
      - なめらかな右肩あがりの曲線となる
    - outputは0から1の数になる。そこにしきい値を設けて、*以上なら○　*以下なら×という判断をする
    - シグモイド関数は、生物の神経細胞が持つ性質をモデル化したものである
      - 単調増加連続関数で、1つの変曲点を持つ
  - フィードフォワードニューラルネットワーク
    - これはネットワーク内にループがない
    - ある層の出力が次の層の入力になる
  - 再帰型ニューラルネットワーク
    - 静止するまでの限られた時間に発火するようなニューロンをもったモデル
    - 発火が他のニューロンを刺激し、そのニューロンもまた限られた時間の中で少し遅れて発火
  - コスト関数
  - Θλ∑½⅓⅔¼¾⅕
  - 平均二乗誤差：RMSE(root mean squared error)
  - SVM：サポートベクターマシン
    - カーネルトリックという方法で、直線で分割できない物を線形で分割できるように写像をつくるらしい
  - その他
    - http://www.cs.ce.nihon-u.ac.jp/~matsui/nn.html
    - 線形分離可能であれば1Layerのパーセプトロンで学習可能らしい
      - XORは線形分離可能ではないので、2層にしないと学習できない
        - 2本の線が必要
      - 線形分離可能：1本の直線で2つのクラスを分離出来ること
      - 現実に登場するほとんどの問題は 線形分離不可能
