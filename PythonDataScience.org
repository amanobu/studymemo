* 資料
http://www.tsjshg.info/udemy/
データ変換はここが詳しい:https://qiita.com/richi40/items/6b3af6f4b00d62dbe8e1

* Python Data Science
- 3～:numpyアレイ
  - plt.imshow(array)
    - ヒートマップみたいなものを表示させる
  - plt.colorbar()
  - plt.title("もじれつ")
  - True,Falseのarrayから値をフィルタ
    - A=np.array([1,2,3,4])
    - B=np.array([1000,2000,3000,4000])
    - conditon=np.array([True,True,False,False])
    - ans = [(a if cond else b) for a,b,cond in zip(A,B,conditon)]
      - zip(~)このまとめで新しいリストを作る
      - [1,2,3000,4000]
      - これだと、遅い＆多次元に対応できない
    - ans2 = np.where(condition,A,B)
      - array([1,2,3000,4000])
  - arr = np.array([[1,2,3],[4,5,6],[7,8,9]])
    - arr.sum():45
    - arr.sum(0):行ごとの和
    - arr.mean():平均
    - arr.std():標準偏差
    - arr.var():分散
  - bool_arr=np.array([True,False,True])
    - bool_arr.any():orですかね
    - bool_arr.all():andですかね
  - arr.sort()
    - 破壊的なソート
  - arr.unique(array)
    - 重複を削る
  - arr.in1d()
    - np.in1d(['France','USA'],arr)
      - 要素ごとにarrに含まれているかを返す
  - アレイの入出力
  - np.save('ファイル名',保存するarray)
    - .npyが付いたファイルが保存される
  - arr = np.load('ファイル名')
    - 読み込みされる
  - np.savez('ファイル名.npz',x=arr1,y=arr2)
    - npzとするのが一般的
    - archive_arry = np.load(～)
    - archive_arry['x']とかで取り出せる
  - np.savetxt(~)
    - text形式で保存される
    - np.loadtxt
  - アレイ
    - import numpy as np
    - np.array(リスト)
    - shape
      - 行列の形式
    - dtype
      - データ型
      - すべて同じ型ではないとＮＧ
    - np.eye
      - 単位行列
    - np.empty(*)
    - np.ones((*,*))
    - np.arange(5,50,2)
  - アレイの添え字
    - arr = arange(0,11)
    - arr[8]
    - arr[1:5]
    - arr[0:5]=100
      - index0~4は100が代入
    - slice_arr = arr[0:6]
    - slice_arr[:]=99
      - こうすると元々のarrも99が0~6まで代入される
      - 参照の為
      - そのようにならないためにはコピーする
        - arr.copy()
    - arr2d=np.array([[5,10,15],[20,25,30],[35,40,45]])
    - arr2d[1]→[20,25,30]
    - arr2d[1][0]→20
      - arr2d[1,0]でも同じ
    - arr2d[:2,1:]
      - [ [10,15][25,30] ]
    - arr2d[2,:]はarr2d[2]と同じ
    - 
    - arr2d=npzeroz((10,10))
    - arr_length = arr2d.shage[1]
    - for i in range(arr_length):
      - arr2d[i]=i #各行がその添え字で上書きされる
      - 
    - この状態で
      - arr2d[ [2,4,5,6] ]
        - これでその行だけとってこれる
        - 順番も変えられる
  - 行と列の入れ替え
    - arr=np.arange(9).reshape(3,3)
    - 転置:以下同じ
      - arr.T
      - arr.transpose()
        - arr.transpose((行,列)):arr.transpose(1,0)で転置
          - arr.swapaxesってのもある
    - np.dot():行列のかけ算
    - 3D
      - arr3d=np.arange(12).reshape((3,2,2))
      - これもおなじく transposeを使える(*,*,*)引数が増える
  - アレイ計算
    - arr=np.arange(11)
    - np.sqrt(arr)
    - np.exp(arr)
    - 
    - A=np.random.randn(10)
      - 正規分布に従う乱数
    - B=np.random.randn(10)
    - np.add(A,B)
      - 各要素毎の加算
    - np.maximum(A,B)
      - 各要素で大きい方を返す
- 14:pandas入門:Seriase
  - import pandas as pd
  - from pandas import Series
  - npのアレイとの違いはデータにインデックスがふられている
  - obj = Series([3,6,9,12])
    - obj.values
      - array([3,6,9,12])
    - obj.index
  - ww2 = Series([1,2,3],index=['a','b','c'])
  - ww2['a']
    - 1
  - ww2[ww2>2]
    - ww2>2とすると
      - 'a' False
      - 'b' False
      - 'c' True
    - が返ってくるのでこれをもとにフィルタをかけるので回答は
      - 'c' 3
  - ww2.to_dict()
    - ディクショナリ型になる
    - Series(ディクショナリ型)でSeriesを作れる
  - pd.isnull(Series)
    - Nanがあるか
    - pd.notnull(～)
  - ww2.name='なまえ'
    - Seriesに名前を付ける
  - ww2.index.name
    - インデックスにも名前を付けられる
- 15:DataFrame
  - form pandas import Series,DataFrame
  - ClipBoardからデータを作れる
  - frame = pd.read_clipboard()
    - これでで！できる
  - frame.columns
    - カラム名全部
  - frame['カラム名']
  - frame.Team
  - frame.[ ['列1','列2'] ]
    - 複数列の抽出
  - DataFrame(frame,columns=[....])
    - 新しいDataFrameを作る
  - frame.head()
    - 先頭５行だけとれる
    - 引数で拡張できる
    - おなじくtailも
  - frame.ix[3]
    - 行のindexを指定して
  - frame['新しい列'] = 'セットするあたい'
  - SeriesからDataFrameも作れる
  - del frame['列']
    - 列が消せる
  - DictoryからDataFrameを作れる
    - DataFrame(dictory)
- 16:DataFrameやSeriesのindex
  - my_ser = Series([1,2,3,4],index=['A','B','C','D'])
  - my_ser
    - A 1
    - B 2
    - ...
  - my_index = my_ser.index
  - my_index[0]
    - 'A'
  - my_index[2:]
    - Index(['C,','D'],dtype=object)
  - インデックスは基本てきには変更不可
    - my_index[0] ='Z'
    - これはNG
- 17:indexを変更する
  - ser1 = Series([1,2,3,4],index = ['A','B','C','D'])
  - ser2 = ser1.reindex(['A','B','C','D','E','F'])
    - indexの再構築
    - E,Fは値が内のでNanが割り当てられる
  - ser2.reindex(['A','B','C','D','E','F'],fill_value=0)
    - 新しいものに値を埋める：この場合0
  - ser3.Series(['USA','Mexico','Canada'],index=[0,5,10])
  - ser3.reindex(range(15),method='ffill')
    - 本来Nanの値が値を埋める
    - ffillは前にむかって埋める問うことで、0の値を採用して1~4を埋める
  - dframe = DataFrame(randn(25).reshape(5,5),index=['A','B','D','E','F'],columns=['col1','col2','col3','col4','col5'])
    - Cがないのは意図的
    - new_index = ['A','B','C','D','E','F']
    - dframe2 = dframe.reindex(new_index)
      - Cが追加されて、値が無し
    - new_columns = ['col1','col2','col3','col4','col5','col6']
      - col6が新しい
      - dframe2.reindex(columns=new_columns)
        - 列が追加された値はNan
    - dframe.ix(new_index,new_columns)
      - 行と列の付け替えが一発でできる
- 18:行の列の削除
  - ser1 = Series(np.arange(3),index=['a','b','c'])
  - ser1.drop('b')
    - bを削除
  - DataFrameでも同じ
    - dframe.drop('index名')
  - 元々のデータは削除されていない
  - 列の削除
    - dframe.drop('列名',axis=1)
      - axis=1は列という意味
      - 行の削除の時はaxis=0が省略されている
- 19:データの取り出し
  - ser1 = Series(np.arange(3),index=['A','B','C'])
  - ser1['B']
    - 1
  - ser1[1]
    - 0番目がAで、1番目がBで....
  - ser[0:3]とかも
  - ser[ ['A','B'] ]
  - ser[ser1 > 2] 条件を入れられる
    - ser[ser1 > 2]　= 新しい値
  - 
  - dframe = DataFrame(np.arange(25).reshape((5,5)),index=['A','B','D','E','F'],columns=['col1','col2','col3','col4','col5'])
  - dframe['B']　列
  - dframe[ [ 'B','C'] ] 複数
  - dframe[ dframe['C'] > 7] 条件式も
  - dframe>10
    - 各要素の条件式に従ったTrue,Falseがかえる
  - dframe.ix['col1']
    - シリーズがかえる
  - dframe[1]
- 20:形が違うデータの計算
  - ser1  = Series([0,1,2],index=['A','B','C'])
  - ser2  = Series([3,4,5,6],index=['A','B','C','D'])
  - ser1+ser2
    - DはNan
  - dframe1 = DataFrame(randn(4).reshape(2,2),index=list('AB'),columns=['NYC','LA'])
  - dframe2 = DataFrame(randn(9).reshape(3,3),index=list('ABC'),columns=['NYC','SF','LA'])
  - dframe1+dframe2
    - 共通するデータがある場合以外はNan
    - これを避ける為には
      - dframe1.add(dframe2,fill_value=0)
        - Nanのところがうまる
  - ser3 = dframe2.ix[0]
    - 0行めを取得
  - dframe2 - ser3
- 21:データの並び替え
  - ser1 = Series(range(3),index=['C','A','B'])
  - ser1.sort_index()　indexの値でソート
    - A 1
    - B 2
    - C 3
    - となる。中身は変わらない
  - ser1.order()
    - こっちは値でソートする
  - ser1.rank()
    - indexの値が何番目かがわかる
  - ser1.sort()
    - これは破壊的に並び替える
- 22:データの統計量
  - arr = np.array([1,2,np.nan],[np.nan,3,4])
  - dframe1 = DataFrame(arr,index=['A','B'],columns=['One','Two','Three'])
  - dframe1.sum()
    - 列のSUM
  - dframe1.sum(axis=1)
    - 行方向のSUM
  - dframe1.min()
    - 列ごとの最小値
  - dframe2.idxmin()
    - 列ごとの最小値
  - dframe1.cumsum()
    - 累積
  - dframe1.describe()
    - データの個数や平均、などなどが一気に計算される
    - 非常に役に立つ模様
  - 株価のデータ解析
    - import pandas_datareader as pdweb
    - import datetime
    - prices = pdweb.get_data_google(['CVX','XOM','BP'],start=datetime.datetime(2010,1,1),end=datetime.datetime(2013,1,1))['Close']
      - ['Adj Close']は終わりを示す記号らしい
      - USAのyahooの株価を取ってくるらしい
    - prices.head()
    - rets = prices.pct_change()
      - 日ごとの変化量を計算
    - prices.plot()
      - プロット
      - jupyterで表示するには、
        - %matplotlib inline
        - を実行すること
    - rets.corr()
      - 相関関係の計算
      - import seaborn as sns
      - import matplotlib.pyplot as plt
      - sns.heatmap(rets.corr())
  - 重複の関係の話
    - ser1 = Series(....
      - ser1.unique()
    - ser1.value_counts()
      - 重複の個数の計算
- 23:欠損値の扱い
  - from numpy import nan
  - data = Series(['one','two',nan,'four'])
  - data.isnull()
    - どこに欠損があるか
  - data.dropna()
    - 欠損が削除される
  - dataframe = DataFrame([[1,2,3],[nan,5,6],[7,nan,9],[nan,nan,nan]])
  - dataframe.dropna()
    - すべて値がある1,2,3だけが残る
  - dataframe.dropna(how='all')
    - すべてが欠損している行が消える
  - dataframe.dropna(axis=1)
    - axis=1：列
    - この場合全部消える：前列に欠損値があるので
  - dataframe2 = DataFrame([[1,2,3,nan],[2,nan,5,6],[nan,7,nan,9],[1,nan,nan,nan]])
  - dataframe2 = dropna(thresh=2)
    - 閾値の指定
    - この場合欠損値が2個以上の行が残る
  - dataframe2.fillna(1)
    - 欠損値のところに値を埋める
    - dataframe2.fillna(1,inplace=True)
      - こうするともともとの値が破壊的に書き換わる
  - dataframe2.fillna({0:0,1:1,2:2,3:3})
    - 0列は0、1列は1で埋める
- 24:indexの階層構造
  - from numpy.random import randn
  - ser = Series(np.random.randn(6) index=[[1,1,1,2,2,2],['a','b','c','a','b','c'] ])
    - こうすると、階層構造になる
    - 1 a ～
    - 1 b ～
    - 1 c ～
    - 2 a ～
    - 2 b ～
    - 2 c ～
  - ser[1]とかくと、1の塊をとってくる
  - ser[:,'a'] とかくと、indexを指定せず'a'のものだけを取ってくる
  - dframe = ser.unstack()
    - インデックスの階層構造がバラバラになり、
  - dframe.unstack()でSeriesに
    - dframe.T.unstack()
      - 元の表と同じ形式にするには転置する必要がある
  - dframe2 = DataFrame(np.arange(16).reshape((4,4)),index=[['a','a','b','b'],[1,2,1,2]],columns=[['NY','NY','LA','SF'],['cold','hot','hot','cold']])
    - 行と列が階層的な構造
  - indexには名前をつけられる
    - dframe2.index.names = ['INDEX_1','INDEX_2']
  - 列にも
    - dframe2.columns.names = ['Cities','Temp']
  - 名前を付けると、入れ替えできる
    - dframe2.swaplevel('Cities','Temp',axis=1)
  - dframe2.sum(level='Temp',axis=1)
    - TempのところでSUM
- 25:テキストデータの読み書き
  - import pandas as pd
  - ｃｓｖ
    - q,r,s,t,apple
    - 2,3,4,5,pear
    - a,s,d,f,rabbit
    - 5,2,5,7,dog
  - dframe = pd.read_csv('ファイル名')
    - 先頭行がヘッダになってしまうので無効にする場合
      - pd.read_csv('file',header=None)
  - pd.read_table('file',sep=',',)
    - sepで区切り文字で読み込む
  - pd.read_csv('file',nrows=2)
    - 読み込む行数を制限:nrows
  - dframe.to_csv('ファイル名')
    - ファイルへの保存
    - import sys
    - dframe.to_csv(sys.stdout)
      - 標準出力へ
  - dframe.to_csv('ファイル名',sep='\t')
    - 保存時の区切り文字の出力
  - dframe.to_csv('ファイル名',columns=[0,1,2])
    - 保存する列の指定
- 26:JSON
  - import json
  - data=json.loads('文字列表現のjson')
    - ディクショナリ型でロード
  - json.dumps(data)
    - ダンプ
  - json.dump(data,open('出力ファイル','w'))
  - json.load('loadするファイル')
- 27:HTMLからのデータ取り出し
  - import pandas as pd
  - url = 'http://www.fdic.gov/bank/individual/failed/banklist.html'
    - テーブルが書かれているページを読み込める
  - dframe_list = pd.io.html.read_html(url)
  - dframe_list[0]
- 28:Excel形式のファイル読み込み
  - import pandas as pd
  - dframe = pd.read_excel('filename',sheetname='シート名')
  - セルが連結されているものはあまり使わないほうがよさそう
- 29:データフレームのマージ
  - import numpy as np
  - import pandas as pd
  - from pandas as DataFrame
  - dframe1 = DataFrame({'key':['X','Z','Y','Z','X','X'],'data_set_1':np.arange(6)})
  - dframe2 = DataFrame({'key':['Q','Y','Z'],'data_set_2':[1,2,3]})
  - pd.merge(dframe1,dframe2)
    - なにも指定しないと共通してある行だけが抽出される
  - pd.merge(dframe1,dframe2,on='key')
    - 'key'で
  - pd.merge(dframe1,dframe2,on='key',how='left')
    - left outer joinですな
  - pd.merge(dframe1,dframe2,on='key',how='outer')
    - どちらかにあれば出てくる
- 30:indexを使ったマージ
  - df_left = DataFrame({'key':['X','Y','Z','X','Y'],'data':range(5)})
  - df_right = DataFrame({'group_data':[10,20]},index=['X','Y'])
  - pd.merge(df_left,df_right,left_on='key',right_index=True)
  - pd.merge(df_left,df_right,left_on='key',right_index=True,how='outer')
  - df_left.join(df_right)
    - joinを使うことが多い
- 31:データの連結
  - arr1=np.arange(9).reshape((3,3))
  - n.concatenate([arr1,arr1],axis=1)
    - 列の方向（右に）連結される
  - n.concatenate([arr1,arr1],axis=0)
    - これは行（下）方向へ
  - ser1 = Series([0,1,2], index=['T','U','V'])
  - ser2 = Series([3,4], index=['X','Y'])
  - pd.concat([ser1,ser2])
    - 行の連結
  - pd.concat([ser1,ser2],axis=1)
    - 列方向
  - pd.concat([ser1,ser2],keys=['cat1','cat2])
  - pd.concat([ser1,ser2],axis=1keys=['cat1','cat2])
  - dframe1 = DataFrame(np.random.randn(4,3),columns=['X','Y','Z'])
  - dframe2 = DataFrame(np.random.randn(3,3),columns=['Y','Q','X'])
  - pd.concat([dframe1,dframe2])
  - pd.concat([dframe1,dframe2],ignore_index=True)
    - もともとのindexを無視して連結
- 32:データの組み合わせる
  - ser1=Series([2,np.nan,4,np.nan,6,np.nan],index=['Q','R','S','T','U','V'])
  - ser2=Series(np.arange(len(ser1),dtype=np.float64),ndex=['Q','R','S','T','U','V'])
  - np.where(pd.isnull(ser1))
    - nanの場所のindexが返ってくる
  - np.where(pd.isnull(ser1),ser2,ser1)
    - nanならばser2の値、それでなければser1の値を取ってくる
  - Series(np.where(pd.isnull(ser1),ser2,ser1),index=ser1.index)
    - 上の説明の通り
  - ser1.combine_first(ser2)
    - 上のと同じことができるser1がnanでなければser1の値。でなければser2の値
  - dframe_odds=DataFrame({'X':[1,np.nan,3,np.nan],'Y':[np.nan,5,np.nan,7],'Z':[np.nan,9,np.nan,11]})
  - dframe_evens=DataFrame({'X':[2,4,np.nan,6,8],'Y':[np.nan,10,12,14,16]})
  - dframe_odds.combine_first(dframe2)
- 33:SeriesとDataFrameの変換
  - ちょいと飛ばそう
- 34:ピボットテーブル
  - dframe.pivod(行,列,埋めるもの)
    - ↑どのような行、列、埋めるものは何がほしいのかによるのでその都度変える
- 35:重複したデータ
  - dframe.duplicated()
    - データが重なっているか？のTrue/Falseを返す
  - dframe.drop_duplicates()
    - 重複データの削除ができる
  - dframe.drop_duplicates(['key'])
    - keyをみて先頭のものを取ってくる
  - dframe.drop_duplicates(['key'],take_last=True)
    - keyをみて一番最後のものを取ってくる
- 36:マッピングを使ったDataFrameへの列の追加
  - dframe=DataFrame({'city':['Alma','BrianHead','FoxPark'],'altitude':[3158,3000,2752]})
  - state_map{'Alma':'Colorad','BrianHead':'Utah','FoxPark':'Wyoming'}
  - dframe['state']=dfame['city'].map(state_map)
    - state_mapをcityをキーに追加
    - dframe['key1']=[0,1,2]で追加できるけど、ある列の値をkeyについかできる
- 37:置換
  - ser1 = Series([1,2,3,4,1,2,3,4])
  - ser1.replace(置き換えるもの,置き換え先のもの)
    - リストで渡せ、いっぺんに置換も
    - ディクショナリ型で{置き換えるもの:置き換え先のもの}でも渡せる
- 38:DataFrameのindexの変更
  - dframe = DataFrame(np.arange(12).reshape((3,4)),index=['NY','LA','SF'],columns=['A','B','C','D'])
  - dframe.index.map(str.lower)
    - str.lowerは小文字になる
    - dframe.index = dframe.index.map(str.lower)
      - これで変更される
  - dframe.rename(index=str.title,columns=str.lower)
    - str.title()は文章の先頭1文字が大文字になる関数)
    - 関数ポインタ？みたいのを渡す
  - dframe.rename(index={'ny':'NEW YORK'}, columns={'A':'ALPHA'})
    - 辞書を引数に、該当するものを変更する
    - inplace=Trueを渡すを破壊的に変更される
- 39:ビニング：データの分類
  - import pandas as pd
  - years = [1900,1991,1992,2008,2012,2015,1987,1969,2013,2008,1999]
  - decate_bins = [1960,1970,1980,1990,2000,2010,2020]
    - 10年ごとに集計してみるための指標
  - decate_cat = pd.cut(years,decate_bins)
    - (はふくまず、]は含む
  - decate_cat.categories
  - pd.value_counts(decate_cat)
    - それぞれのカテゴリにデータが何個あるか
  - pd.cut(years,2)
    - 全体が2グループに分けられる
  - でも大体ヒストグラムの機能にこれらgあ含まれるので...使わないかもしれない
- 40:外れ値
  - import numpy as np
  - import pandas import DataFrame
  - np.random.seed(12345)
    - 引数を同じ値を与ええらば、同じ乱数が得られる
  - dframe = DataFrame(np.random.randn(1000,4))
  - col = dframe[0]
    - col[np.abs(col)>3]
      - 絶対値が3以上のものだけを取り出す
  - np.abs(dframe)>3
    - この結果条件に合うかのTrue/Falseが返却
  - dframe[(np.abs(dframe)>3).any(1)]
    - 1はaxisの方向(列)
    - anyはどれかの列にTrueがあるか
      - →どこかの列に3より大きいものがある
  - np.sign(dframe)
    - それぞれの符号(-1/1)が返ってくる
  - dframe[np.abs(dframe)>3] = np.sign(dframe)*3
    - その場所がマイナスならば-3、プラスなら+3される
- 41:Permutation：ランダムに順列をバラバラにする
  - import numpy as np
  - import pandas import DataFrame
  - dframe = DataFrame(np.arange(4*4).reshape((4,4)))
  - blender = np.array([0,3,2,1])
  - dframe.take(blender)
    - 0行目は変わらない
    - 1行目が3行目となる
    - 次は2行目
    - 最後は1行目
  - blender = np.random.permutation(4)
    - array([2,3,1,0])みたいなものがランダムにもらえるので、これをdframe.take(blender)でランダムな行列になる
  - いままではあったデータをそうさしていたが、こんどはデータを取り出してはもとに戻す
  - box = np.array(['A','B','C'])
  - shaker = np.random.randint(0,len(box),size=10)
    - ０から2までの値で10このarray
  - hand_grabs = box.take(shaker)
    - A,B,Cのなかから、取り出した結果の配列のイメージ
  - シミュレーションのデータイメージ
- 42:DataFrameのGroupBy
  - import numpy as np
  - import pandas as pd
  - from pandas import DataFrame
  - dframe = DataFrame({'k1':['X','X','Y','Y','Z'],'k2':['alpha','beta','alpha','beta','alpha'],'dataset1':np.random.randn(5),'dataset2':np.random.randn(5)})
  - group1 = dframe['dataset1'].groupby(dframe['k1'])
    - dataset1の列についてk1の列についてまとめてみる
  - cities = np.array(['NY','LA','LA','NY','NY'])
  - month = np.array(['JAN','FEB','JAN','FEB','JAN'])
  - dframe['dataset1'].groupby([cities,month])
    - dataset1に対してもともとdataframeにはないものに対して処理できる
    - この場合同じものは、0番めのNY,JANと４番目のNY,JANなので、indexがそのものが出てくる
  - dframe.groupby(['k1','k2'])
    - 複数の列でまとめる場合リストを渡せばできる
  - dataset2_group = dframe.groupby(['k1','k2'])[ ['dataset2'] ]
    - 列を限定するdataset2だけとなる
  - dframe.groupby(['k1']).size()
    - それぞれのグループに何個あるか？
    - for name,group in dframe.groupby('k1'):
      - ...繰り返しで取得
  - gr = dframe.groupby('k1')
  - gr.get_group('X')
    - XのDataFrameを取得
  - いままでは行方向でgroupbyしたが列にもできる
    - 複雑でわからないのでメモしない
- 43:GroupByその2
  - import numpy as np
  - import pandas as pd
  - from pandas import Series, DataFrame
  - animals = DataFrame(np.arange(16).reshape(4,4),columns=['W','X','Y','Z'],index=['Dog','Cat','Bird','Mouse'])
  - animals.is[1:2,['W','Y']] = np.nan
    - 1:2 つまり1行目のW,Yをnanに
  - behavior_map = {'W':'bad','X':'good','Y':'bad','Z':'good'}
  - animals_col = animals.groupby(behavior_map, axis=1)
    - WとYはbadなのでひとまとまり
    - XとZはひとまとまり
  - ↑これをDictonaryではなくSeriesでもできる
  - behavior_series = Series(behavior_map)
  - animals.groupby(behavior_series, axis=1)
    - 同じことができた
  - groupbyには関数をあたえられる
  - animals.groupby(len)
    - lenはindexの文字列の長さでgroupbyされる
  - animals.groupby([len,['A','B','A','B']])
    - A,B,A,Bのところがよくわからんけどこんなことができるらしい
- 44:データのAggregation:たくさんあるデータから特徴的なデータ(max,mean,minなど)を抽出する
  - import numpy as np
  - import pandas as pd
  - from pandas import Series, DataFrame
  - url='http://archive.ics.uci.edu/ml/machine-lerning-database/wine-quality'
    - ワインの質に関係するサンプルデータ
    - セミコロンで区切られている
    - 1行が一本のワインを表している
  - dframe_wine = pd.read_csv(ファイル名,sep=';')
  - def max_to_min(arr):
    - return arr.max() - arr.min()
  - wino = dframe_wine.groupby('quality')
  - wino.agg(max_to_min)
    - qualityでグルーピングした各列に対してmax_to_minを計算する
  - wino.agg('mean')
    - 文字列も渡せてこの場合平均値を計算してくれる
  - dframe_wine['qual/alc raito'] = dframe_wine['quality'] /dframe_wine['alcohol']
    - ↑新しい列を追加することは簡単
  - dframe_wine.pivot_table(index=['quality'])
    - groupbyしたときと同じ値をとれる
  - 可視化の件
  - dframe_wine.plot(kind='scatter', x='quality',y='alcohol')
    - 散布図
- 45:Split,Apply,Combine
  - Split:分割group by
  - Apply:分割された毎に何か計算(平均値とか)
  - Combine:連結して結果を表示
  - import numpy as np
  - import pandas as pd
  - from pandas import Series, DataFrame
  - 44のワインのデータをつかう
  - dframe_wine = pd.read_csv(ファイル名,sep=';')
  - qualityごとにアルコール度数が高いものを出す
  - def ranker(df): #アルコール度数のランク付け
    - df['alc_content_rank'] = np.arange(len(df)) + 1
    - return df
  - dframe_wine.sort('alcohol',ascending=False,inplace=True)
    - ascending=Falseは降順
    - 破壊的にソート:inplace=True
  - dframe_wine = dframe_wine.groupby('quality').apply(ranker)
  - num_of_qual = dframe_wine['quality'].value_counts()
    - それぞれのデータが何個あるか？
  - dframe_wine[dframe_wine.alc_count_rank==1]
- 46:クロス集計
  - import pandas as pd
  - import io import StringIO #文字列をファイルのように読み書きする
  - data = '''Sample Animal Intelligense
    1 Dog   Dumb
    2 Dog Dumb
    3 Cat       Smart
    4 Cat    Smart
    5 Dog Smart
    6 Cat  Smart'''
    - スペースは適当で構わない
  - dframe = pd.read_tables(StringIO(data), sep='\s+')
    - \s+は正規表現で空白1回以上の繰り返し
  - pd.crosstab(dframe.Animal,dframe.Intelligense)
    - クロス集計表
  - pd.crosstab(dframe.Animal,dframe.Intelligense,margins=Ture)
    - 行と列ごとに合計を追加してくれる
- 47:Seaborn
  - 非常に優れたデータ可視化ライブラリ
    - 色を簡単に変えられる
- 48:ヒストグラム
  - from numpy.randm import randn
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - %matplotlib inline(jupyter用)
  - dataset1 = randn(100)
    - 正規分布
  - plt.hist(dataset1)
    - デフォルト10区切り
  - dataset2 = randn(80)
  - plt.hist(dataset2, color='indianred')
    - 色を変えた
  - plt.hist(dataset1, normed=True)
    - 面積をすべて足すを１になるようにする（標準化：形式をあわせられる）
    - 何がいいかというと、重ねられる↓
  - plt.hist(dataset1, normed=True, alpha=0.5, bins=20)
  - plt.hist(dataset2, normed=True, alpha=0.5, bins=20, color='indianred')
  - 上記を同時実行する(jpyterで)
    - 重なって表示される
  - data1 = randn(1000)
  - data2 = randn(1000)
  - sns.jointplot(data,1,data2) #結合分布,結合分布というもの
    - sns.jointplot(data,1,data2,kind='hex')
      - pointではなく、色の濃さの６角形で表現される
  - 未知のデータを見た時にヒストグラムで表現するのが初めの手段
- 49:カーネル密度推定
  - 超簡単に言うと、なめらかなヒストグラムを作る
  - 別の細かい関数の足し合わせでなめらかな曲線を描く
  - seabornを使うと超絶簡単に
  - import numpy as np
  - from numpy.random import randn
  - from scipy import stats
  - import mathplotlib as mpl
  - import mathplotlib.pyplot as plt
  - import seaborn as sns
  - %matplotlib inlne
  - dataset = randn(25)
  - sns.rugplot(dataset)
    - データがあるところに線が
  - plt.hist(dataset,alpha=0.3)
  - sns.rugplot(ataset)
    - 重ねてみるといい感じになっているかと
  - BandWidthSelection
    - wikipediaの値を採用してみる
  - sns.rugplot(dataset)
  - x_min = dataset.min() - 2
  - x_man = dataset.max() + 2
  - x_axis = np.linspace(x_min,x_max),100)
  - ↑最大値と最小値を100等分した軸を作る
  - bandwidth = ((4*datasset.std()**5)/(3*len(dataset)))**0.2
    - ↑wikipediaの式をそのまま
  - kernel_list = []
  - for data_point in dataset: #ポイント事にkernelを作成
    - kernel = stats.norm(data_point, bandwidth).pdf(x_axis)
    - kernel_list.append(kernel)
    - kernel = kernel / kernel.max()
    - kernel = kernel * 0.4
    - plt.plot.(x_axis, kernel ,color = 'gray' , alpha=0.5)
  - plt.ylim(0,1)
  - sum_of_kde = np.sum(kernel_list,axis=0)　#カーネルの足し合わせる
  - fig = plt.plot(x_axis,sum_of_kde ,color='indianred)
  - sns.rugplot(dataset)
  - plt.yticks([]) #y軸に空をわたして消している
  - plt.suptitle('Sum of the Basis Functions') #日本語を書くと文字化ける。回避方法は資料に
  - ↑これをseabornをつかうと１行でできる！↓
  - sns.kdeplot(dataset)
  - 説明
  - sns.rugplot(dataset,color='black')
  - for bw in np.arange(0.5,2,0.25)
    - sns.kdeplot(dataset,bw=bw,label=bw)
  - ↑bandwidthの幅によってどのような影響があるか
    - 狭いとぐぐぐ！と高くなり、広いと裾が広がる
  - ↑はガウス分布を使ったが、他にどのような物があるか
  - kernel_options = ['biw','cos','gau','tri','triw']
  - for kern in kernel_options:
    - sns.kdeplot(dataset,kernel=kern,label=kern)
  - ↑いろいろな規定関数の差がわかる
  - 累積分布関数
  - plt.hist(dataset,cumulative=True)
  - ↑これもがたがたになるので、
    - sns.kdeplot(dataset,cumulative=True)
    - cumulative積み上げいく
  - ２次元でも出来る
  - mean=[0,0] #原点を平均
  - cov=[ [1,0], [0,100] ]　#それぞれの方向への分散
  - dataset2 = np.random.multivariate_normal(mean,cov,1000) #ランダムに1000点
  - dframe = pd.DataFrame(dataset2,columns=['X','Y'])
  - sns.kdeplot(dframe)
    - 軸ごとにデータもわたせて
    - sns.kdeplot(dframe.X,dframe.Y,shade=True)
  - バンド幅も
    - sns.kdeplot(dframe,bw=1)
  - 他の推定方法も
    - sns.kdeplot(dframe,bw='silverman')
      - くわしくはドキュメントを
  - sns.jointplot('X','Y',dframe,kind='kde')
- 50:分布の可視化
  - from numpy.random import randn
  - import seaborn as sns
  - %matplotlib inline
  - dataset = randn(100)
  - sns.distplot(dataset)
    - カーネル密度推定と、ヒスとグラムも
  - sns.distplot(dataset,rug=True,hist=False)
    - 表示するグラフを選択出来る
  - sns.distplot(dataset,bins=25,kde_kws={'color:indeanred,'label':'KDE PLOT'})
    - カーネル密度推定の色を変える
  - Seriesと親和性が高いPandasと
  - from pandas import Series
  - ser1 = Series(dataset,name='My_DATA',)
  - sns.distplot(ser1)
- 51:box port,ヴァイオリンプロット
  - import numpy as np
  - from numpy.random import randn
  - from scipy import stats
  - import seaborn as sns
  - %matplotlib inline
  - data1 = randn(100)
  - data2 = randn(100)+2
  - sns.boxplot(data=[data1,data2])
    - はこひげ図
    - 真ん中が中央値
    - 箱は25~75%の所
    - 外れ値がある（あまりにも大きい・小さい値）
    - 外れ値を含む長いひげを描く場合
      - sns.boxplot(data=[data1,data2],whis=np.inf)
    - 横に描画
      - sns.boxplot(data=[data1,data2],orient='h')
      - 縦はorient='v'
  - バイオリンプロット
  - data1 = stats.norm(0,5)rvn(100)
    - 平均が０標準偏差が5の100
  - data2 = np.concatenate([stats.gamma(5).rvs(50) -1 ,-1 *stats.gamma(5).rvs(50)])
  - sns.violinplot(data=[data1,data2])
    - sns.boxplot(data=[data1,data2])
    - と形は同じだが、violinplotを描いてみると全然違う
  - カーネル密度推定と同じような書き方なのでバンド幅を変えられる
    - sns.violinplot(data=[data1,data2],bw=0.01)
  - sns.violinplot(data=data1,inner='stick')
    - バイオリンプロットのなかでデータがどこにあるか示せる
- 52:回帰とプロット
  - import numpy as np
  - from numpy.random import randn
  - from scipy import stats
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - %matplotlib inline
  - seaboanにはサンプルデータも含まれている
  - tips = sns.load_dataset('tips')
    - チップのデータ
  - sns.lmplot('total_bill','tip',tips)
    - 回帰直線を書く
      - tipsの名前のDataFrameから
      - X:totalbill
      - Y:tip
    - 薄く色がついているところは信頼区間と呼ばれている
  - sns.lmplot('total_bill','tip',tips,scatter_kws={'marker':'o','color':'indianred'},line_kws={'linewidth':1,'color':'blue'})
    - 色付け
  - sns.lmplot('total_bill','tip',tips,order=4,scatter_kws={'marker':'o','color':'indianred'},line_kws={'linewidth':1,'color':'blue'})
    - order=*で高次元の多項式でフィットさせる
    - おおお！
  - sns.lmplot('total_bill','tip',tips,fit_reg=False)
    - 回帰線を描かない：fit_reg
  - tips['tip_pect'] = 100*(tips['tip']/tips['total_bill'])
    - 新しい列を追加
      - 総額のいくら払ったかの列
  - sns.lmplot('size','tip_pect',tips)
    - sizeは連続じゃなく、離散的な値
    - それもうまく簡単にかける
  - sns.lmplot('size','tip_pect',tips,x_jitter=0.2)
    - jitter揺らぎ？？
    - ばらける
  - sns.lmplot('size','tip_pect',tips,x_estimator=np.mean)
    - チップのパーセントの平均値
      - 2,3,4人の払う額は安定している：グラフから
  - sns.lmplot('total_bill','tip_pect',hue='sex',markers=['x','o']
    - hueで指定した列をmarkersでかく
  - sns.lmplot('total_bill','tip_pect',tips,lowess=True)
    - lowess
    - 回帰直線が少しなめらかにするととう
    - 局所的にデータを見て線を引く
  - sns.regplot('total_bill','tip_rect',tips)
    - lmportはregplotを呼んでいる
  - seaboan自体はmatprotlibを呼んでいる
  - 二つのグラフを並べて書く
    - fig,(axis1,axis2) = plt.subplots(1,2,sharey=True)
      - 1行2列の描画領域を取得(axis1,axis2)
      - shareyでY軸を共有する
    - sns.regplot('total_bill','tip_pect',tips,ax=axis1)
    - sns.violinplot(y='tip_pect' x='size',data=tips.sort('size'),ax=axis2)
    - matplotlibとseaboanは親和性が高い
- 53:ヒートマップとクラスタリング
  - import numpy as np
  - from numpy.random import randn
  - from scipy import stats
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - %matplotlib inline
  - またサンプルを使う
  - flight_dframe = sns.load_dataset('flights')
  - flight_dframe = flight_dframe.pivod('month','year','passengers')
    - データ形式変更
  - sns.heatmap(flight_dframe)
    - おおおお！
  - sns.heatmap(flight_dframe,annot=True,fmt='d')
    - ヒートマップに数字が上書き！
  - sns.heatmap(flight_dframe,center=flight_dframe.loc['January',1955])
    - 1955年1月を基準に色を変えてみる
  - 二つのグラフを書いてみる
    - f,(axis1,axis2) = plt.subplots(2,1)
    - yearly_flights = flight_dframe.sum()
      - 12か月ごとの合計値
    - years = ps.Series(yearly_flights.index.values)
    - years = pd.DataFrame(years)
    - flights = pd.Series(yearly_flights.values)
    - flights = pd.DataFrame(flights)
    - year_dframe = pd.concat((years,fights),axis=1)
    - year_dframe.columns = ['Year','Flights']
    - 
    - sns.barplot('Year', y='Flights',data=year_dframe,ax=axis1)
    - sns.heatmap(flight_dframe,cmap='Blues',ax=axis2,cbar_kws={'orientation':'horizontal'})
  - クラスタ化したヒートマップ
    - sns.clustermap(flight_dframe)
      - 行ごと、列ごとに近いものが集まる
  - sns.clustermap(flight_dframe,col_cluster=False)
    - 列方向のクラスタリングをしない
  - データの標準化
    - sns.clustermap(flight_dframe,standard_scale=1)
      - 1は列方向
      - 0は行方向
  - zスコア：例のzスコア
    - sns.clustermap(flight_dframe,z_score=1)
- 56～:Titanic号の沈没解析
  - import numpy as np
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - import pandas as pd
  - from pandas import Series, DataFrame
  - $matplotlib inline
  - まず性別
    - sns.countplot('Sex',data=titanic_df)
      - 男性が多い
    - sns.countplot('Sex',data=titanic_df,hue='Pclass')
      - 客室ランクでわけ
    - sns.countplot('Pclass',data=titanic_df,hue='Sex')
      - 客室ランクで性別
  - 子供のデータを入れる。年齢から分ける
    - def male_female_child(passenger):
      - age, sex = passenger
      - if age < 16:
        - return 'child' #16歳未満子供ならchild
      - else:
        - reurn sex #大人であればどちらかの性別
    - 新しい列を作る
    - titanic_df['person'] = titanic_df[ ['Age','Sex] ].apply(male_female_child, axis=1)
      - 年齢と性別の列をとってきて関数に渡し、personの列ができる
    - sns.countplot('Pclass',data=titanic_df,hue='pserson')
      - 大人の男女・こどもが
  - titanic_df['Age'].hist(bins=70)
    - 年齢別ヒストグラム描画
    - titanic_df['Age'].mean() #平均年齢
    - titanic_df['person'].value_counts()
      - 男女・子供のそれぞれのカウント
  - カーネル密度推定で描いてみる
    - fig = sns.FacetGrid(titanic_df, hue='person', aspect=4)
      - aspect:表の幅
    - fig.map(sns.kdeplot, 'age', shade=True)
    - oldest = titanic_df['Age'].max()
    - fig.set(xlim=(0,oldest))
      - 0歳からmax年齢まで
    - figadd_legend()
    - 結果、客室等級がさがるにわかい
  - どこにいたか
    - deck = titanic_df['Cabin'].dropna()
      - A～Gみたいなものあどの階にいたか
    - levels = []
    - for level in deck:
      - levels.append(level[0])
    - cabin_df = DataFrame(levels)
    - cabin_df.columns = ['Cabin']
    - ↑これを使ってどこにどれぐらいの人がいたか
    - sns.countplot('Cabin', data=cabin_df, palette='winter_d', order=sorted(set(levels)))
      - levelsをSetオブジェクトにして並び替えて置く
    - cabin_df = cabin_df[cabin_df.Cabin != 'T']
      - T階？は誤りだと思われるので削除する
  - どこから乗ってきたか
    - Embarked:港
    - 乗った港と客室ランクの関係
      - sns.countplot('Embarked', data=titanic_df, hue='Pclass')
  - from collections import Counter
  - Counter(titanic_df.Embarked)
    - 港のカウント
    - 実行してみると判るがnanがある
      - seaboanなどのフレームワークはnanを意識させないようになっている
    - 時々nanを意識しないと合計などがおかしくなると思う
  - 家族属性
    - titanic_df['Alone'] = titanic_df.Parch + titanic_df.SibSp
      - Parch：両親or子供と一緒だったか（なにか値があれば
      - SibSp：兄弟姉妹と一緒か（なにか値があれば
    - titanic_df['Alone'].loc[titanic_df['Alone']>0] = 'With Family'
    - titanic_df['Alone'].loc[titanic_df['Alone']==0] = 'Alone'
      - ここでエラーがおきるかもしれないが問題ないらしい
      - 0よりおおきければ家族と、0ならひとり
    - sns.countplot('Alone' ,data = titanic_df, palette = 'Blues')
      - 単身者が多かったことが
  - 生存率
    - titanic_df.df['Survivor'] = titanic_df.Survived.map({0:'no',1:'yes})
      - 0は未生存、1は生存
      - sns.countplot('Survivor', data = titanic_df, plalette = 'Set1')
    - 客室のランクと生存者の関係
      - sns.factplot('Pclass', 'Survived', data = titanic_df, order=[1,2,3])
        - 1等客室の生存が高い
    - 女性や子供を先にというポリシーはどうなんだ？3等は圧倒的に男がおおい
    - sns.factplot('Pclass', 'Survived', hue='person', data = titanic_df, order=[1,2,3],aspect=2)
      - 男性は生存率が低い
      - 女性と子供に関しては1等も2頭もあまり変わらない
      - 3等は全体的に生存率が低い
    - 生存率と年齢の関係
      - sns.lmplot('Age', 'Survived', data = titanic_df)
        - 高齢になるほど生存率が下がっている
      - sns.lmplot('Age', 'Survived', hue='Pclass' data = titanic_df, hue_order=[1,2,3])
        - やっぱり1等が一番高い
      - generations = [10,20,40,60,80]
      - sns.lmplot('Age', 'Survived', hue='Pclass' data = titanic_df, hue_order=[1,2,3], x_bins=generations)
        - 年代ごとにプロットして、標準偏差を縦棒で書いてくれる
          - 80代の高齢の生存率がとびぬけている
      - sns.lmplot('Age', 'Survived', hue='Sex' data = titanic_df, x_bins=generations)
        - 性別で分類
        - 女性は年齢が上がるごとに生存率が上がっていた
        - 80代の男が生存率が高い
  - まとめ
    - hue（層別化）は非常に役に立つ
    - ゴミが時々あるので削除する必要がある
    - ほかの疑問もぜひやってみて
- 60～株式市場のデータ解析
  - pandasはこんな金融情報を扱うために作られたらしい
  - 時系列データの扱いが主
  - import pandas as pd
  - from pandas import Series, DataFrame
  - import numpy as np
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - sns.set_style('whitegrid') #背景
  - %matplotlib inline
  - from pandas.io.data import DataReader
    - アメリカの株式市場の株価は対応。日本はイケてないらしい
  - from datetime import datetime
  - tech_list = ['AAPL', 'GOOG', 'MSFT', 'AMZN']
    - 日本は４桁のコードだが、アメリカは上のようなもの
  - end = datetime.now()
    - 終わりはいつまでか
  - start = datetime(end.year - 1, end.month, end.day)
    - ちょうど１年まから今日まで
  - for stock in tech_list:
    - globals()[stock] =  DataReader(stock, 'yahoo', start, end) 
  - globalsはプログラムのコードを変数でおきかえられる
    - type(AAPL):DataFrameになる
    - APPL.describe() という感じでアクセスできる
    - pythonの標準機能
  - 説明：Open:はつね、HIgh：一番高値、Close：終値、Volume：出来高、AdjCloseなんかで調整した終値
  - AAPL['Adj Close'].plot(legend = True, figsize=(10,4))
  - AAPL['Volume'].plot(legend = True, figsize=(10,4))
  - 
  - 移動平均
    - 10日とかの平均をグラフ化すること
    - ma_day = [10,20,50]
    - for ma in ma_day:
      - column_name = 'MA {}'.format(ma)
      - AAPL[column_name] = pd.rolling_mean(AAPL['Adj Close'],ma) #新しい列
    - AAPL[ ['Adj Close', 'MA 10','MA 20','MA 50'] ].plot(subplots=False,figsize(10,4)
      - 50日平均は遅れてくる
    - AAPL['Daily Return'] = AAPL['Adj Close'].pct_change()
      - 1日毎の終値がどれだけ前日に比べ変化したかの列を追加
    - AAPL['Daily Return'].plot(figsize=(10,4), legent=True,linestyle='--' marger='o')
    - sns.distplot(AAPL['Daily Return'].dropna(), bins=100,color='purple')
      - 1年間でみるとどうなるのか？
    - 複数会社のと扱う
    - closing_df = DataReader(['AAPL','GOOG','MSFT','AMZN'],'yahoo',start,end)['Adj Close]
    - tech_rets = closing_df.pct_change()
      - 1日の移動平均データ
    - sns.jointplot('GOOG','GOOG',tech_rets, kind='scatter', color='seagreen')
      - 同じ会社で比較
    - sns.jointplot('GOOG','MSFT',tech_rets, kind='scatter', color='seagreen')
      - google,MS
      - 非常によく相関していることが分かる
        - piasonの相関係数
    - ピアソンの相関係数：
      - 相関が全くないと０
      - どちらかが増えるとどちらが増える：正の値
      - どちらかが増えるとどちらかが減る：負の値
      - 数字だけで判断するのは気を付けなければならない
        - 傾きだけじゃわからない。多少正なら、１とはんだん？
        - 資料参照
  - 複数の会社比較
    - sns.pairplot(tech_rets.dropna())
      - 対角線上は同じ会社
    - returns_fig = sns.PairGrid(tech_rets.dropna())
    - returns_fig.map_upper(plt.scatter, color='purple') #スキャッタープロット
    - returns_fig.map_lower(sns.kdeplot, cmap='cool_d') #カーネル密度推定
    - returns_fig.map_diag(plt.hist, bins=30)
    - ↑グラフの表示場所の変更
    - sns.heatmap(tech_rets.corr(), annot=True)
      - 日々の変動との相関
      - 相関が高い！　どっかの会社があがれば、あがる
  - 株式リスク解析
    - リスクとリターンの図示
    - rets = tech_rets.dropna()
      - nanを取り除く
    - plt.scatter(rets.mean(),rets.std(), alpha=0.5, s=np.pi*20)
      - 横が期待される収益：リターン
      - 縦は株価の変動標準偏差：リスク
    - plt.ylim([0.01,0.025])
      - 縦軸のサイズ
    - plt.xlim([-0.005,0.01])
      - 横
    - plt.xlabel('Expeced returns')
    - plt.ylabel('Risk')
    - 図に説明を追加する
    - for label, x, y, in zip(rets.columns, rets.mean(), rets.std()):
      - plt.annotate(label, xy=(x,y), xytext=(0,50), textcoords = 'offset points',ha='right',va='bottom', arrowprops=dict(arrowstyle='-', connectionstyle='arc3')) #図にアノテーションを追記する
    - リスクの見積もり
      - Value at Risk
        - sns.distplot(AAPL['Daily Return'].dropna(), bins=100)
          - 日々のリターンが変化したか？という事の図
      - パーセンタイル
        - rets['AAPL'].quantile(0.05)
        - この返り値
          - ちいさいほうから5%が
      - この辺が意味がわからない★
      - 
      - １年後の株価を予測する：ブラウン運動
        - ランダムな動きの積み重ねのモデル
        - 乱数を使ってシミュレーションをする事をモンテカルロ法という
        - 細かいことは資料にあり
      - days = 365
      - dt = 1/days
      - mu = rets.mean()['GOOG']
        - 日ごとの平均の変動値
      - sigama = rets.std()['GOOG']
        - 1日の１年間の変動の標準偏差
      - def stock_monte_carlo(start_price, days, mu, sigma): #これは資料にあるブラウン運動モデルの関数
        - price = np.zeros(days)
        - price[0] = start_price
        - shock = np.zeros(days) #shock:資料に説明あり　数式の一部の事らしい下記driftも同じ
        - drift = np.zeros(days)
        - for x in range(1, days):
          - shock[x] = np.random.nomal(loc=mu*dt, scale=sigma * np.sqrt()) #正規分布に従う乱数
          - drrft[x] = mu * dt
          - price[x] = price[x-1] + (price[x-1] * (drift[x]+shock[x])) #ひとつ前のデータをもとに株価の計算
        - return price
        - 
        - start_price = GOOG.iloc[0,5] #最初の価格は最初のAdj Closeの値
        - for run in range(5): #5回ほど１年後の株価を計算する
          - plt.plot(stock_monte_carlo(start_price, days, mu ,sigma))
        - 
        - 
        - runs = 10000 #10000回シミュレーション
        - simulations = np.zeros(runs) #結果のSTORE先
        - np.set_printoptions(threshold=5)
        - for run in range(runs):
          - simulations[run] = stock_monte_carlo(start_price, days, mu ,sigma)[days-1]
        - plt.hist(simulations,bins=200)
        - 上位1％を見積もる
        - q = np.percentile(simulations,1)
        - plt.hist(simulations, bins=200)
        - plt.figtext(0.6, 0.8, s='Start price: {:0.2}.format(start_price))
        - plt.figtext(0.6, 0,7, 'mean final price: {:0.2}.format(simulations.mean()))
        - plt.figtext(0.6, 0.6, 'ValueAtRisk(0.99): {:0.2f}.format(start_price-q)) #この値が損する額
        - plt.figtext(0.15, 0.6, 'q(0.99): {:0.2f}'.format(q)) #上位1%の境目の価格
        - plt.axvline(x=q, linewidth=4, color='r')
        - ※あまりよくわからなかったので後で再度やったほうが良いかもしれない
- 66:選挙の解析
  - 世論調査のデータと、寄付のデータ２つで解析をする
  - import pandas as pd
  - from pandas import Series, DataFrame
  - import numpy as np
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - %matplotlib inline
  - request:便利なHTTP Client
  - StringIO:これも便利な何か
  - import requests
  - from io import StringIO
  - url = 'http://elections.huffingtonpost.com/pollster/2012-general-election-remney-vs-obama.csv'
  - source = requests.get(url).text
  - poll_data = StringIO(source) #ファイルのように扱える
  - poll_df = pd.read_csv(poll_data) #DataFrame作成
  - poll_df[ [[['Pollster','Partisan','Affiliation']]] ].sort('Pollster').drop_duplicates()
    - 各党の調査の主体になっている機関でソート
    - afficeationに支持政党
  - 調査対象に政党色があることを調べる
  - sns.countplot('Affiliation', data=poll_df)
    - ほとんど中立の立場が多い
  - sns.countplot('Affiliation', data=poll_df, hue='Population', order=['Rep','Dem','None'])
    - Population:調査対象
  - それぞれの支持がどれぐらい集まっていたか
  - avg = pd.DataFrame(poll_df.mean())
  - avg.drop('Number of Observations', axis=0,inplace=True)
    - Number of Observationsはゴミみたいなので削除する
  - std = pd.DataFrame(poll_df.std())
  - std.drop('Number of Observations', axis=0,inplace=True)
  - avg.plot(yerr=std, kind='bar', legend=False)
    - 棒グラフにerrバーが付く
  - データをまとめておく
  - poll_avg = pd.concat([avg,std], axis=1)
  - poll_avg.columns = ['Average', 'STD']
  - 
  - poll_df.plot(x='End Date', y=['Obama','Romney', 'Undecided'], marker='o',linestyle='')
    - 最後のあたりは接戦になっている
  - from datetime import datatime
  - poll_df['Difference']=(poll_df.Obama - poll_df.Romney)/100
    - ＋ｏｂａｍａリード、ーはロムニーさんリード
    - 列を追加
  - poll_df = poll_df.groupby(['Start Date'], as_index=False).mean()
    - as_index=false:いまとおなじ0～ではじまるindexを保持するため
    - 同じ日の世論調査の結果の表ができあがる
  - fig = poll_df.plot('Start date','Difference',figsize(12,4),marker='o',linestyle='-')
    - 支持率の差をプロット
  - 2012年10月3と、10月11,10/22に討論会があった。１０月分をプロットしてみる
  - poll_df[poll_df['Start Date'].apply(lambda x:x.startswith('2012-10'))]
    - start dataが'2012-10'で始まるかどうか
  - fig = poll_df.plot('Start date','Difference',figsize(12,4),marker='o',linestyle='-', xlim=(329,356))
  - plt.axvline(x=330, linewidth=4, color='gray') #討論会の日に縦線
  - plt.axvline(x=337, linewidth=4, color='gray') #討論会の日に縦線
  - plt.axvline(x=347, linewidth=4, color='gray') #討論会の日に縦線
    - (329,356)は2012年10月のindexの値
  - 
  - 寄付のデータ解析
  - データは資料にあり
  - donor_df = pd.read_csv('Election_Donor_Data.csv')
    - 警告が出るがむし（カラムによっていろんなデータがミックスされているみたいな）
  - 寄付金額後との件数
  - donor_df['contb_receipt_amt'].value_counts()
    - 金額事にその件数をカウント
    - マイナスもある：寄付の払い戻しらしい
  - donor_df['contb_receipt_amt'].value_counts().shape
    - 種類の件数
  - don_mean = donor_df['contb_receipt_amt'].mean()
  - don_std = donor_df['contb_receipt_amt'].std()
  - print('平均{:0.2f} 標準偏差{:0.2f}'.format(don_mean, don_std))
    - 標準偏差がおかしい値
      - マイナスの数字があったりするので。。。
  - top_donor = donor_df['contb_receipt_amt'].copy()
  - top_donor.sort()
    - 結果、開きがあるので標準偏差がすごい値になる
  - top_donor = top_donor[top_donor > 0] #マイナス削除する
  - top_donor.sort()
  - top_donor.value_counts.head(10)
    - トップ10寄付金額の種類の多さ
  - com_don = top_donor[top_donor < 2500] #2500doll以下の物を見て見る
  - com_don.hist(bins=100)
  - 
  - 候補者の所属政党事の寄付
  - candidates = donor_df.cand_nm.unique() # 候補者のリストをとる
  - party_map ={....資料にあり...。名前:政党....}
  - donor_df['Party'] = donor_df.cond_nm.map(party_map)
    - 候補者の名前に応じて政党がセット
  - donor_df = donor_df[donor_df.contb_receipt_amt> 0] #マイナスを消す
  - donor_df.groupby('cand_nm')['contb_receipt_amt'].count()
    - 候補者毎に何件寄付があるか
  - cand_amount = donor_df.groupby('cand_nm')['contb_receipt_amt'].sum()
    - 候補者毎の寄付額
  - cand_amount.plot(kind='bar')
    - 候補者毎の寄付額のグラフ
  - donor_df.groupby('Party')['contb_receipt_amt'].sum().plot(kind='bar')
    - 政党毎に寄付額
    - 民主党は一人しか出さないらしい
    - 共和党は複数
  - occupation_df = donor_df.pivot_table('contb_receipt_amt',index='countbr_occupation',columns='Party',oggfunc='sum')
    - oggfunc='sum':まとめる処理。この場合は合計
    - 職業毎、政党ごとの寄付金額
    - occupation_df.shapeが45000行ぐらいある笑
  - occupation_df = occupation_df[occupation_df.sum(1)>1000000]
    - ちょっと寄付金額で絞って100万ドル以上で
    - C.E.OとかCEOとか揺らぎがある
    - occupation_df.drop(['～',...],axis=0,inplace=True)
      - 不要なものを↑で消す
    - occupation_df.loc['CEO'] = occupation_df.loc['CEO'] + occupation_df.loc['C.E.O']
      - 一つにまとめる
    - occupation_df.drop(['C.E.O'], inplace=True)
      - こっちは削除
  - occupation_df.plot(kind='barh',figsize(10,12) cmap='sismic')
    - 横の棒グラフ
    - 退職者の寄付がおおい
    - 共和党はCEOが支持しているみたいなことがわかる
- 71:SciKitLernによる機械学習入門
  - 説明変数：パラメータ
  - 目的変数：解
- 72:線形回帰
  - データの準備
  - import numpy as np
  - import pandas as pd
  - from pandas import Series,DataFrame
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - %matplotlib inline
  - from sklearn.datasets import load_boston #ボストンの住宅データ
  - boston = load_boston()
  - boston_df = DataFrame(boston.data)
  - boston_df.columns = boston.feature_names
  - boston_df['Price'] = boston.target
  - sns.lmplot('RM','Price', data = boston_df)
    - RM:部屋の数
    - この線の関数を求める
- 73:線形回帰２
  - まず変数１つでやってみる
  - 2次元ARRAYにする必要があるので
  - X = np.vstack(boston_df.RM)
  - Y = boston_df.Price
  - y = ax + b
    - から y = Ap
      - ベクトルの内積する必要があるらしい SciKitLern の仕様
      - A  = [x 1] #横
      - p = [a,b]#縦
  - [ [ value, 1 ] for value in X]
  - X = np.array([ [ value, 1 ] for value in X])
  - a ,b = np.linalg.lstsq(X,Y)[0]
    - np.linalg.lstsq(X,Y)の返りの0個目の要素をa,b
  - plt.plot(boston_df.RM,boston_df.Price, 'o')
  - x = boston_df.RM
  - plt.plot(x, a*x+b, 'r')
    - seabornで書いた線と同じような線を引いてみる
- 74:線形回帰３
  - result = np.linalg.listsq(X,Y)
  - error_total = result[1]
    - 先ほど[0]の値を使ったが、[1]には誤差の全体のsumが入っている
  - rmse = np.sqrt(error_total/len(X))
    - 平均で割って平方根(最小二乗法の２じょうからもとの次元にもどす)：平均時2乗誤差の平方根
    - この値でどのぐらい当てはまっているかの指標となる。
      - 値は6.60：標準偏差とイメージが一緒なので、2倍の-13.2~+13.2の間に95%のデータが入る
        - 正規分布の例の表にしがたう
  - 複数パラメータの線形回帰:SciKitLernをつかう
    - import sklearn
    - from sklearn.linear_model import LinearRegression
    - lreg = LinearRegression()
    - 簡単な使い方
      - lreg.fit()はデータをもとにモデルを作る
      - lreg.predict()は作られたモデルをもとに予測値を返す
    - 
    - X_multi = boston_df.drop('Price',1)
      - 目的変数なので削る
    - Y_target = boston_df.Price
    - lreg.fit(X_multi, Y_target)
      - これだけ
        - これで→のb,a1,a2,a3...が求まる： y = b + a1x1+a2x2+....
    - lreg.intercept_
      - bにあたる数字
    - lreg.coef
      - 係数が何個あるか:a1...a13
    - coeff_df = DataFrame(boston_df.columns)
      - 枠だけを作る
    - coeff_df.clumuns = ['Features']
    - coeff_df['Coefficient Estimate'] = pd.Series(lreg.coef)
      - それぞれの説明変数に対する係数
- 75:線形回帰５
  - サンプルを分ける
  - CrossValidationSetとTrainingSetに分ける
  - X_train, X_test, Y_train, Y_test = sklearn.cross_validation.train_test_split(X_multi,boston_df.Price)
    - おお、これ一発！
  - lreg = LinearRegression()
  - lreg.fit(X_train,Y_train) #トレーニングSETでモデルを作る
  - pred_train = lreg.predict(X_train)
  - pred_test = lreg.predict(X_test)
  - 誤差を計算:平均二乗誤差
    - np.mean(Y_train - pred_train) ** 2)
    - np.mean(Y_train - pred_test) ** 2)
  - 残差プロット：誤差と正しい値の差の図を書いてみる
  - train = plt.scatter(pred_train, (pred_train - Y_train), c='b', alpha=0.5)
  - test = plt.scatter(pred_test, (pred_test - Y_test), c='r', alpha=0.5)
  - plt.hlines(y=0,xmin=-1.0,xmax=50)
  - plt.legend((train,test),('Traning','Test'),loc='lower left') #凡例
  - plt.title('Residual Plots)
  - 注目点：
    - y=0の横線に乗るのがいい：予測が一致したから
    - y>0とy<0の大体の数が同じぐらいになるのがいい
- 76:ロジスティック回帰
  - Statsmodelsというモジュールが必要
  - import numpy as np
  - import pandas as pd
  - from pandas import Series,DataFrame
  - import math
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - %matplotlib inline
  - from sklearn.linear_modem import LogisticRegression
  - from sklearn.cross_validation import train_test_split
  - from sklearn import metrics
  - import statsmodels.api as sm
  - シグモイド関数をplotしてみる
  - def ligistic(t):
    - return 1.0/(1+math.exp(-1.0*t))
  - t = np.linspace(-6.6,500)
  - y = np.array([logistic(ele) for ele in t])
  - plt.plot(t,y)
- 77:ロジスティック回帰２
  - 不倫データの解析ｗ
  - df = sm.datasets.fair.load_pandas().data
  - affairsにはいっている値は不倫関係に使った時間なので、0以外は1とする
  - def affair_check(x):
    - if x != 0:
      - return 1
    - else:
      - return 0
  - df['Had_Affair'] = df['affairs'].apply(affair_check)
    - 不倫の有無を0,1でに変換
  - データの傾向を見る
    - df.groupby('Had_Affair').mean()
    - sns.countplot('age', data=df.sort('age'), hue=('Had_Affair'), palette='coolwarm')
      - 若いとそうでもないが...
    - sns.countplot('yrs_married', data=df.sort('yrs_marrie'), hue=('Had_Affair'), palette='coolwarm')
    - sns.countplot('children', data=df.sort('children'), hue=('Had_Affair'), palette='coolwarm')
- 78:ロジスティック回帰
  - データの前処理
  - 2つだけ違う性質の列がある
    - 職業：ほかの値はどんどん値が強くなるが、職業のところはただ種類のだけ
    - なのでダミー変数に置き換える
  - occ_dummies = pd.get_dummies(df.occupation)
  - hus_occ_dummies = pd.get_dummies(df.occupation_husb)
    - だんなの職業
  - occ_dummies = ... 詳しくは資料で
  - hus_occ_dummies = ... 詳しくは資料で
  - X = df.drop(['occupation','occupation_husb','Had_Affair'],axis=1)
    - 変換した列と目的変数を削除
  - dummies = pd.concat([occ_dummies, hus_occ_dummies], axis=1)
  - X = pd.concat([X,dummies],axis=1)
    - 変換した値を追加
  - Y = df.Had_Affair
  - 多重共線性
    - 例えば男女をmale,femaleのダミー変数を導入したとする
    - male=1ならfemake=0になる
    - maleとfemaleは非常に高い相関
      - これを多重共線性というらしい
    - このようなダミー変数を回帰モデルを含めるのはよくない
    - これを今回のものから取り除く
      - X = X.drop('occ1',axis=1)
      - X = X.drop('hocc1',axis=1)
  - X = X.drop('affairs', axis=1)
    - もともと Had_Affairの元ネタなので消す
  - Y = Y.values
    - または、Y = np.ravel(Y)
    - 次で使うので1次元の配列にしておく
- 79:ロジスティック回帰
  - log_model = LogisticRegression()
  - log_model.fit(X,Y)
    - これでできたｗ
  - log_model.score(X,Y)
    - どれぐらいの制度で予測できるかのスコア
  - coeff_df = DataFrame([X.columns,log_model.coef_[0]]).T
    - モデルの説明変数の係数の表示
      - 負の値は不倫率が下がる、正はあがる
  - またcrossvalidation SETとtraining SETにわける
  - X_train,X_test,Y_train,Y_test = train_test_split(X,Y)
  - log_model2 = LogisticRegression()
  - log_model2.if(X_train,Y_train)
  - class_predit = log_model2.predict(X_test)
  - metris.accuracy_score(Y_test,class_predict)
    - これで正解率
- 80:他クラス分類
  - 花びらの長さと、幅などからアヤメの種類を特定するというデータセット
  - 1対nの方法で
  - import numpy as np
  - import pandas as pd
  - from pandas import Series,DataFrame
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - from sklearn.datasets import load_iris
  - iris = load_iris()
  - X = iris.data
  - Y = iris.target
  - iris_data = DataFrame(X, columns = ['Sepail Length','Sepal Width', 'Petal Length','Petal Width'])
    - とりあえずDataFrameに
  - iris_target = DataFrame(Y, columns=['Species'])
  - def flower(num): 花の種類を文字列に変換
    - if num == 0:
      - return 'Setosa'
    - elif num == 1:
      - return 'Veriscolour'
    - else:
      - return 'Virginica'
  - iris_target['Species'] = iris_target['Species'].apply(flower)
  - iris = pd.concat([iris_data, iris_target], axis=1)
    - データをまとめる
  - sns.pairplot(iris,hue = 'Species',size=2)
    - 1対2みたいになっているのでまず１つは分類は簡単だろう
    - いろんな図をみると２つの似ているやつが分かれているものがあるのでそれで分類できるかもしれない
      - という予想
  - sns.countplot('Petal Length',data=iris)
- 81:多クラス分類
  - from sklearn.linear_model import LogisticRegression
  - from sklearn.cross_validation import train_test_split
  - logreg = LogisticRegression()
  - X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.4,random_state=3)
    - 全体の40%をテストにする
    - random_state=3と固定するといつも同じデータとなる
  - logreg.fit(X_train,Y_train)
  - from sklearn import metrics
  - Y_pred = logreg.predict(X_test)
  - metrics.accuracy_score(Y_test,Y_pred)
    - 正答率
    - 特になにもしなくても、裏で他クラス分類されている！
  - K近傍法
    - 新しいサンプルが来た時に
    - 最も近くにあるK個の値を見る
    - その値を見て、多数決で新しいサンプルのclassを決める
  - from sklearn.neighbors import KNeighborsClassfier
  - knn = KNeighborsClassfier(n_neighbors = 6)
    - k=６とした
  - knn.fit(X_train,Y_train)
  - Y_pred = knn.predict(X_test)
  - metrics.accuracy_score(Y_test,Y_pred)
    - 正答率
  - Kの値が変わることで予測値が変わる.plotしてみる
    - k_range = range(1,90)
    - accuracy = []
    - for k in k_range:
      - knn = KNeighborsClassfier(n_neighbors = k)
      - knn.fit(X_train,Y_train)
      - Y_pred = knn.predict(X_test)
      - accuracy.append(metrics.accuracy_score(Y_test,Y_pred))
- 83:SVM
  - 分離する境界が線じゃなくて2次以上の面？
    - 本当か？
    - マージンを取る
  - ある次元で分離できなかったものを別空間写像して分割することを試みる：カーネルトリック
    - 動画が資料にリンクされているので見たほうがよさそう
- 84:SVM2
  - import numpy as np
  - import matplotlib.pyplot as plt
  - from sklearn import datasets
  - iris = datasets.load_iris()
  - X = iris_data
  - Y = iris_target
  - from sklearn.svm import SVC
  - model = SVC()
  - from sklearn.cross_validation import train_test_split
  - X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state=0)
  - model.fit(X_train,Y_train)
  - predict = model.predict(X_test)
  - from sklearn import metrics
  - metrics.accuracy_score(Y_test,predict)
  - sklearnのdefaultのKernelはRBFっていうやつらしい
- 84:ナイーブベイズ分類
  - Π：Σの掛け算バージョン
  - arg max f(x)
    - f(x) = 1 - |x| の場合、arg max f(x) = {0}
  - ナイーブベイズ
    - スパムメール分類に思いに使われている機械学習アルゴリズム
    - y:目的変数
    - x1～xn:説明変数
    - P(y|x1....xn) = P(y)P(x1....xn|y) / P(x1...xn)
    - すべての説明変数が互いに独立としているとすると
    - P(y|x1....xn) = P(y)　Π P(xi|y) / P(x1...xn)
      - iは1からnまで
      - こう示せるので計算が簡単になる
    - P(y|x1....xn)：そのクラスに属する確率が返ってくる
      - で、arg max で最も大きな確率に割り当たるクラスに分類する
  - ガルシアンナイーブベイズ
    - 説明変数の連続値が正規分布に従うものとしてモデル化すると、計算が楽になる
- 85:ナイーブベイズ分類2
  - import pandas as pd
  - from pandas import Series,DataFrame
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - from sklearn import datasets
  - from sklearn import metrics
  - from sklearn.naive_bayes import GaussianNB
  - iris = datasets.load_iris()
  - X = iris.data
  - Y = iris.target
  - model = GaussianNB()
  - from sklearn.cross_validation import train_test_split
  - X_train,X_test,Y_train,Y_test = train_test_split(X,Y,random_state=0)
  - model.fit(X_train,Y_train)
  - predicted = model.predict(X_test)
  - metrincs.accuracy_score(Y_test,predicted)
- 86:決定木とランダムフォレスト
  - 説明変数から判断条件によってばわい分けをしていく木
  - 決定木をいくつもつくって集めるのでランダムフォレスト
  - import pandas as pd
  - import nampy as np
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - from sklearn.datasets import make_blobs
    - ダミーデータ生成用
  - X,y = make_blobs(n_samples=500,centers=4,random_state=8,cluster_std=2.4)
    - ダミーデータを作成
    - centers:目的変数の何個の分類か？
    - cluster_std:データのばらつきのパラメータ
  - plt.figure(figsize=(10,10))
  - plt.scatter(X[:,0], X[:,1],c=y, s=50, cmap='jet)
  - from sklearn.tree import DecisionTreeClassifier
  - 関数定義は資料にあり
  - clf =  DecisionTreeClassifier(max_depth=2,random_state=0)
    - Treeの深さは2まで
  - visualize_tree(clf,X,y)
  - clf =  DecisionTreeClassifier(max_depth=4,random_state=0)
    - 深さ４にしてみる...がoverfitting
  - overfittingを防ぐ方法として、ランダムフォレストらしい
  - from sklearn.ensemble import RandomForestClassifier
  - clf = RandomForestClassifier(n_eismatars=100,random_state=0)
    - ランダムに100Tree作って試行する
  - visualize_tree(clf,X,y,boundaries=False)
    - boundaries=False:ランダムフォレストなのでしていするらしい
  - 分類につかったが、回帰にも使える
  - from sklearn.ensemble import RandomForestRegressor
  - x = 10 * np.random.rand(100)
  - def sin_model(x,sigma=0.2): #大きな波小さな波＋ノイズデータの生成
    - noise = sigma * np.random.randn(len(x))
    - return np.sin(5*x)+np.sin(0.5+x)+noize
    - y = sin_model(x)
  - plt.figure(figsize(16,8))
  - plt.errorbar(x,y,0.1,fmt='o')
    - 描いてみればわかるが、直線にfitさせるのはむずかしい
  - xfit = np.linspace(0,10,1000)
    - xの値を用意
  - rfr = RandomForestRegressor(100)
    - なかの木は100
  - frf.fit(x[:None],y)
    - 波データの学習
  - yfit = rfr.predict(xfit[:,None])
    - 予測値の計算
  - 描画してみる
  - ytrue = sim_model(xfit,0)
    - 実際の値を計算し
  - plt.figure(figsize(16,8))
  - plt.errobar(x,y,0.1,fmt='o')
  - plt.plot(xfit,yfit,'-r') #予測したあたいを赤
  - plt.plot(xfit,ytrue,'-k',alpha=0.5) #実際の値
  - ということで、ランダムフォレストは回帰にも使える
- 90:離散一様分布
  - 確率変数Xがn個の値を同じ同じ確率の時、Xは離散一様分布にしたがう
  - 例：さいころの目
  - import numpy as np
  - import numpy.random import randn
  - import pandas as pd
  - from scipy import stats
  - import matplotlib as mpl
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - 仮想的なサイコロをつくりふるまいを示してみる
  - roll_options = [1,2,3,4,5,6]
  - tprob = 1 #確率の総和は１
  - prob_roll = tprob / len(roll_options)
  - uni_plot = plt.bar(roll_options,[prob_roll] + 6)
  - 離散一様分布の場合平均は、最大＋最小/2
  - 分散：平均からのずれの総和
  - Scipyを使うと離散一様分布が簡単に作れる
  - from scipy.stats import randint
  - low,high=1,7
  - mean ,var = randint.stats(low,high)
  - 応用例として、ドイツの戦車の生産台数を推定する
  - 確率質量関数
    - 離散的な確率変数がある値となる確率の関数
    - 例えば、N回試行したときの確率は？みたいなときを求める関数の事だと思う
- 91:連続一様分布
  - よくわからないのであとで復習
- 92:二項分布
  - Binomial distribution
  - 離散分布の一種
  - scipy.misc.comb で組み合わせを求める関数があるｗ
    - 手計算でも出来るのに。。。。
  - 学生の時やったわ
- 93:ポアソン分布
  - ある間隔や一定期間の間におこるイベントの回数に注目するもの
  - scipy.stats.poisson.pmf(問いの値,平均)で計算できる
  - また、～回以上の確率を・・・みたいなときは確率の足し合わせをする
  - 1 - poisson.cdf(問いの値,平均)で計算できる
    - poisson.cdfは足し合わせる値を計算
- 94:正規分布
  - 特に平均が０、標準偏差が１の正規分布を標準正規分布という
    - ±1の標準偏差で68%,±2の標準偏差に95%,±3までに99.7%が含まれる
- 95:標本と母集団
  - 全体（母集団）のなかから1000人選んで（標本）
  - 無作為抽出と乱数
    - 母集団から同じ確率でサンプルを抽出
    - コンピュータの乱数は本当の乱数ではなく疑似乱数（アルゴリズムで乱数に見立てている）
      - メルセンヌ・ツイスタ法
  - 復元抽出と非復元抽出
    - 非復元抽出：抽出したら母集団に戻さない
    - 復元抽出：母集団に戻す
  - 標本分布の平均
    - ？？？意味わからん。飛ばす
  - 標本の差と和
    - ？？？意味わからん。飛ばす
- 96:t分布
  - うーん....
  - 推定値で確率密度関数が定義されるが、それは、推定値なので正規分布に従わないらしい
  - ただ、数が多いと正規分布に漸近するとのこと
- 97:仮説検定
  - ある仮説が正しいか検討するステップ
    1. データの収集
    2. 前提条件
    3. 仮説
       - 帰無仮説：これが棄却（NG）ならば、対立仮説が採用
       - 対立仮説：上の逆
    4. 検定統計量
       - 正規分布に従うならzスコアで判断
    5. 統計量の分布
    6. 決定のためのルール
       - 有意水準
         - 有意水準を5%とするとを95%の自信で帰無仮説を棄却するという意味
         - 経験的に１、５％が採用されることがおおい
       - True Possitive,False nagative....のあたりも関係
    7. P値の計算
       - どれくらいの自信をもって仮説を棄却できるかの数値
  - 例：
    - FFチェーンで社長は顧客の平均年齢は30歳だと思っているが、現場は違うと思っている
    - 調べたところ１０人の平均が２７歳
    - 分散が２０だとわかっているとする
    - 帰無仮説が30歳、対立仮説がnot30歳
    - zスコアを計算:-2.12
    - 標準正規分布となるので-2.12は95%以上で帰無仮説を否定できる
    - P値：1.7%となり...現場が正しい...
  - この検定方法はベースの部分でこれを応用したものがいっぱいある
- 98:カイ二乗の値
  - 100回coinを投げて50おもて、50裏が普通だが、その後差（ズレ）はどのぐらいまでよいのか？ということ
  - 観測値と予測値の差を足していくイメージ
  - 自由度（？）によって分布がかわる
    - 自由度が増えると、ピーク（ずれ？）がどんどん増える
  - カイ二乗適合度判定
    - 2個のサイコロを500回分の合計値を記録
    - 論理値も計算できる
    - そこからP値を計算してサイコロに不正がないか。。。を検定できる
- 99:ベイズの定理
  - P(A|B):Bが起こった条件でAが起こる確率
  - ベイズの定理
    - P(A|B) = P(B|A)P(A)  / P(B)
  - 例：
    - ボール１にはバニラクッキーが30、チョコレートが10個
    - ボール２バニラクッキー、チョコレートが20個ずつ
    - この時、どちらかのボールからクッキーを取り出した。そのクッキーがバニラだった場合、ボール１を選んだ確率は？
      - V:バニラクッキーを選んだ事象
      - B1ボール1を選ぶ事象
      - P(B1|V) = P(V|B1)P(B1) / P(V)
      - P(V|B1) = 30/40
      - P(B1) = 1/2
      - P(V) = 50/80
      - よってP(B1|V) = 3/5
- 100:SQL
  - Pandasとの連携で、SQLAlchemyが便利
  - SQLiteのデータ形式の閲覧用にSQLite Broswerが便利
  - sakilaデータベース
    - サンプルデータベース：レンタルビデオのデータ
  - import sqlite3
  - import pandas as pd
  - con = sqlite3.connect('sakila.db')
  - sql_query = 'select * from customer'
  - df = pd.read_sql(sql_query,con)
    - おー！DataFrameになってくる
- 101:SQL SELECT
  - とくになし
- 103:Webスクレイピング
  - BeautifulSoup
  - lxml
  - requests
  - 以上が必要
  - from bs4 import BeautifulSoup
  - import requests
  - import pandas as pd
  - from pandas import Series,DataFrame
  - tebleになっているものを取ってくるとする
  - url = '～'
  - result = requests.get(url)
  - c = result.content
    - cにはhtmlの生データ
  - soup = BeautifulSoup(c,'lxml')
  - summary = soup.find('div',{'class':'クラス名','id':'id名})
    - htmlタグの場所を特定
  - tables = summary.find_all('table')
    - これでテーブルが1つならばデータとれる
- 104:Webスクレイピング2
  - 続き
  - data = []
  - rows = tables[0].find_all('tr')
  - for tr in rows:
    - cols = tr.find_all('td')
    - for td in cols:
      - text = td.find(text=True)
      - data.append(text)
  - あとはdataを成形していく
  - 
- 追加
  - DataFrameの行毎の繰り返し
    - for index,row in df.iterrows():
  - DataFrameの行毎に関数の適用df.apply(lambda x:apply_func(x) , axis=1)
    - 各値にはapplymap()
  - pandas.read_csvする時に、勝手に型変換されて0001->1になってしまうときは
    - read_csv(...,dtype={'対象の列':'データタイプ'}) とする

