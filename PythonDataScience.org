* Python Data Science
- section3:numpyアレイ
  - plt.imshow(array)
    - ヒートマップみたいなものを表示させる
  - plt.colorbar()
  - plt.title("もじれつ")
  - True,Falseのarrayから値をフィルタ
    - A=np.array([1,2,3,4])
    - B=np.array([1000,2000,3000,4000])
    - conditon=np.array([True,True,False,False])
    - ans = [(a if cond else b) for a,b,cond in zip(A,B,conditon)]
      - zip(~)このまとめで新しいリストを作る
      - [1,2,3000,4000]
      - これだと、遅い＆多次元に対応できない
    - ans2 = np.where(condition,A,B)
      - array([1,2,3000,4000])
  - arr = np.array([[1,2,3],[4,5,6],[7,8,9]])
    - arr.sum():45
    - arr.sum(0):行ごとの和
    - arr.mean():平均
    - arr.std():標準偏差
    - arr.var():分散
  - bool_arr=np.array([True,False,True])
    - bool_arr.any():orですかね
    - bool_arr.all():andですかね
  - arr.sort()
    - 破壊的なソート
  - arr.unique(array)
    - 重複を削る
  - arr.in1d()
    - np.in1d(['France','USA'],arr)
      - 要素ごとにarrに含まれているかを返す
  - アレイの入出力
  - np.save('ファイル名',保存するarray)
    - .npyが付いたファイルが保存される
  - arr = np.load('ファイル名')
    - 読み込みされる
  - np.savez('ファイル名.npz',x=arr1,y=arr2)
    - npzとするのが一般的
    - archive_arry = np.load(～)
    - archive_arry['x']とかで取り出せる
  - np.savetxt(~)
    - text形式で保存される
    - np.loadtxt
- 14:pandas入門:Seriase
  - import pandas as pd
  - from pandas import Series
  - npのアレイとの違いはデータにインデックスがふられている
  - obj = Series([3,6,9,12])
    - obj.value
      - array([3,6,9,12])
    - obj.index
  - ww2 = Series([1,2,3],index=['a','b','c'])
  - ww2['a']
    - 1
  - ww2[ww2>2]
    - ww2>2とすると
      - 'a' False
      - 'b' False
      - 'c' True
    - が返ってくるのでこれをもとにフィルタをかけるので回答は
      - 'c' 3
  - ww2.to_dict()
    - ディクショナリ型になる
    - Series(ディクショナリ型)でSeriesを作れる
  - pd.isnull(Series)
    - Nanがあるか
    - pd.notnull(～)
  - ww2.name='なまえ'
    - Seriesに名前を付ける
  - ww2.index.name
    - インデックスにも名前を付けられる
- 15:DataFrame
  - form pandas import Series,DataFrame
  - ClipBoardからデータを作れる
  - frame = pd.read_clipboard()
    - これでで！できる
  - frame.columns
    - カラム名全部
  - frame['カラム名']
  - frame.Team
  - frame.[ ['列1','列2'] ]
    - 複数列の抽出
  - DataFrame(frame,columns=[....])
    - 新しいDataFrameを作る
  - frame.head()
    - 先頭５行だけとれる
    - 引数で拡張できる
    - おなじくtailも
  - frame.ix[3]
    - 行のindexを指定して
  - frame['新しい列'] = 'セットするあたい'
  - SeriesからDataFrameも作れる
  - del frame['列']
    - 列が消せる
  - DictoryからDataFrameを作れる
    - DataFrame(dictory)
- 21:データの並び替え
  - ser1 = Series(range(3),index=['C','A','B'])
  - ser1.sort_index()　indexの値でソート
    - A 1
    - B 2
    - C 3
    - となる。中身は変わらない
  - ser1.order()
    - こっちは値でソートする
  - ser1.rank()
    - indexの値が何番目かがわかる
  - ser1.sort()
    - これは破壊的に並び替える
- 22:データの統計量
  - arr = np.array([1,2,np.nan],[np.nan,3,4])
  - dframe1 = DataFrame(arr,index=['A','B'],columns=['One','Two','Three'])
  - dframe1.sum()
    - 列のSUM
  - dframe1.sum(axis=1)
    - 行方向のSUM
  - dframe1.min()
    - 列ごとの最小値
  - dframe2.idxmin()
    - 列ごとの最小値
  - dframe1.cumsum()
    - 累積
  - dframe1.describe()
    - データの個数や平均、などなどが一気に計算される
    - 非常に役に立つ模様
  - 株価のデータ解析
    - import pandas_datareader  as pdweb
    - import datetime
    - prices = pdweb.get_data_google(['CVX','XOM','BP'],start=datetime.datetime(2010,1,1),end=datetime.datetime(2013,1,1))['Close']
      - ['Adj Close']は終わりを示す記号らしい
      - USAのyahooの株価を取ってくるらしい
    - prices.head()
    - rets = prices.pct_change()
      - 日ごとの変化量を計算
    - prices.plot()
      - プロット
      - jupyterで表示するには、
        - %matplotlib inline
        - を実行すること
    - rets.corr()
      - 相関関係の計算
      - import seaborn as sns
      - import matplotlib.pyplot as plt
      - sns.heatmap(rets.corr())
  - 重複の関係の話
    - ser1 = Series(....
      - ser1.unique()
    - ser1.value_counts()
      - 重複の個数の計算
- 23:欠損値の扱い
  - from numpy import nan
  - data = Series(['one','two',nan,'four'])
  - data.isnull()
    - どこに欠損があるか
  - data.dropna()
    - 欠損が削除される
  - dataframe = DataFrame([[1,2,3],[nan,5,6],[7,nan,9],[nan,nan,nan]])
  - dataframe.dropna()
    - すべて値がある1,2,3だけが残る
  - dataframe.dropna(how='all')
    - すべてが欠損している行が消える
  - dataframe.dropna(axis=1)
    - axis=1：列
    - この場合全部消える：前列に欠損値があるので
  - dataframe2 = DataFrame([[1,2,3,nan],[2,nan,5,6],[nan,7,nan,9],[1,nan,nan,nan]])
  - dataframe2 = dropna(thresh=2)
    - 閾値の指定
    - この場合欠損値が2個以上の行が残る
  - dataframe2.fillna(1)
    - 欠損値のところに値を埋める
    - dataframe2.fillna(1,inplace=True)
      - こうするともともとの値が破壊的に書き換わる
  - dataframe2.fillna({0:0,1:1,2:2,3:3})
    - 0列は0、1列は1で埋める
- 24:indexの階層構造
  - from numpy.random import randn
  - ser = Series(np.random.randn(6) index=[[1,1,1,2,2,2],['a','b','c','a','b','c'] ])
    - こうすると、階層構造になる
    - 1 a ～
    - 1 b ～
    - 1 c ～
    - 2 a ～
    - 2 b ～
    - 2 c ～
  - ser[1]とかくと、1の塊をとってくる
  - ser[:,'a'] とかくと、indexを指定せず'a'のものだけを取ってくる
  - dframe = ser.unstack()
    - インデックスの階層構造がバラバラになり、
  - dframe.unstack()でSeriesに
    - dframe.T.unstack()
      - 元の表と同じ形式にするには転置する必要がある
  - dframe2 = DataFrame(np.arange(16).reshape((4,4)),index=[['a','a','b','b'],[1,2,1,2]],columns=[['NY','NY','LA','SF'],['cold','hot','hot','cold']])
    - 行と列が階層的な構造
  - indexには名前をつけられる
    - dframe2.index.names = ['INDEX_1','INDEX_2']
  - 列にも
    - dframe2.columns.names = ['Cities','Temp']
  - 名前を付けると、入れ替えできる
    - dframe2.swaplevel('Cities','Temp',axis=1)
  - dframe2.sum(level='Temp',axis=1)
    - TempのところでSUM
- 25:テキストデータの読み書き
  - import pandas as pd
  - ｃｓｖ
    - q,r,s,t,apple
    - 2,3,4,5,pear
    - a,s,d,f,rabbit
    - 5,2,5,7,dog
  - dframe = pd.read_csv('ファイル名')
    - 先頭行がヘッダになってしまうので無効にする場合
      - pd.read_csv('file',header=None)
  - pd.read_table('file',sep=',',)
    - sepで区切り文字で読み込む
  - pd.read_csv('file',nrows=2)
    - 読み込む行数を制限:nrows
  - dframe.to_csv('ファイル名')
    - ファイルへの保存
    - import sys
    - dframe.to_csv(sys.stdout)
      - 標準出力へ
  - dframe.to_csv('ファイル名',sep='\t')
    - 保存時の区切り文字の出力
  - dframe.to_csv('ファイル名',columns=[0,1,2])
    - 保存する列の指定
- 26:JSON
  - import json
  - data=json.loads('文字列表現のjson')
    - ディクショナリ型でロード
  - json.dumps(data)
    - ダンプ
  - json.dump(data,open('出力ファイル','w'))
  - json.load('loadするファイル')
- 27:HTMLからのデータ取り出し
  - import pandas as pd
  - url = 'http://www.fdic.gov/bank/individual/failed/banklist.html'
    - テーブルが書かれているページを読み込める
  - dframe_list = pd.io.html.read_html(url)
  - dframe_list[0]
- 28:Excel形式のファイル読み込み
  - import pandas as pd
  - dframe = pd.read_excel('filename',sheetname='シート名')
  - セルが連結されているものはあまり使わないほうがよさそう
- 29:データフレームのマージ
  - import numpy as np
  - import pandas as pd
  - from pandas as DataFrame
  - dframe1 = DataFrame({'key':['X','Z','Y','Z','X','X'],'data_set_1':np.arange(6)})
  - dframe2 = DataFrame({'key':['Q','Y','Z'],'data_set_2':[1,2,3]})
  - pd.merge(dframe1,dframe2)
    - なにも指定しないと共通してある行だけが抽出される
  - pd.merge(dframe1,dframe2,on='key')
    - 'key'で
  - pd.merge(dframe1,dframe2,on='key',how='left')
    - left outer joinですな
  - pd.merge(dframe1,dframe2,on='key',how='outer')
    - どちらかにあれば出てくる
- 30:indexを使ったマージ
  - df_left = DataFrame({'key':['X','Y','Z','X','Y'],'data':range(5)})
  - df_right = DataFrame({'group_data':[10,20]},index=['X','Y'])
  - pd.merge(df_left,df_right,left_on='key',right_index=True)
  - pd.merge(df_left,df_right,left_on='key',right_index=True,how='outer')
  - df_left.join(df_right)
    - joinを使うことが多い
- 31:データの連結
  - arr1=np.arange(9).reshape((3,3))
  - n.concatenate([arr1,arr1],axis=1)
    - 列の方向（右に）連結される
  - n.concatenate([arr1,arr1],axis=0)
    - これは行（下）方向へ
  - ser1 = Series([0,1,2], index=['T','U','V'])
  - ser1 = Series([3,4], index=['X','Y'])
  - pd.concat([ser1,ser2])
    - 行の連結
  - pd.concat([ser1,ser2],axis=1)
    - 列方向
  - pd.concat([ser1,ser2],keys=['cat1','cat2])
  - pd.concat([ser1,ser2],axis=1keys=['cat1','cat2])
  - dframe1 = DataFrame(np.random.randn(4,3),columns=['X','Y','Z'])
  - dframe2 = DataFrame(np.random.randn(3,3),columns=['Y','Q','X'])
  - pd.concat([dframe1,dframe2])
  - pd.concat([dframe1,dframe2],ignore_index=True)
    - もともとのindexを無視して連結
- 32:データの組み合わせる
  - ser1=Series([2,np.nan,4,np.nan,6,np.nan],index=['Q','R','S','T','U','V'])
  - ser2=Series(np.arange(len(ser1),dtype=np.float64),ndex=['Q','R','S','T','U','V'])
  - np.where(pd.isnull(ser1))
    - nanの場所のindexが返ってくる
  - np.where(pd.isnull(ser1),ser2,ser1)
    - nanならばser2の値、それでなければser1の値を取ってくる
  - Series(np.where(pd.isnull(ser1),ser2,ser1),index=ser1.index)
    - 上の説明の通り
  - ser1.combine_first(ser2)
    - 上のと同じことができるser1がnanでなければser1の値。でなければser2の値
  - dframe_odds=DataFrame({'X':[1,np.nan,3,np.nan],'Y':[np.nan,5,np.nan,7],'Z':[np.nan,9,np.nan,11]})
  - dframe_evens=DataFrame({'X':[2,4,np.nan,6,8],'Y':[np.nan,10,12,14,16]})
  - dframe_odds.combine_first(dframe2)
- 33:SeriesとDataFrameの変換
  - ちょいと飛ばそう
- 34:ピボットテーブル
  - dframe.pivod(行,列,埋めるもの)
    - ↑どのような行、列、埋めるものは何がほしいのかによるのでその都度変える
- 35:重複したデータ
  - dframe.duplicated()
    - データが重なっているか？のTrue/Falseを返す
  - dframe.drop_duplicates()
    - 重複データの削除ができる
  - dframe.drop_duplicates(['key'])
    - keyをみて先頭のものを取ってくる
  - dframe.drop_duplicates(['key'],take_last=True)
    - keyをみて一番最後のものを取ってくる
- 36:マッピングを使ったDataFrameへの列の追加
  - dframe=DataFrame({'city':['Alma','BrianHead','FoxPark'],'altitude':[3158,3000,2752]})
  - state_map{'Alma':'Colorad','BrianHead':'Utah','FoxPark':'Wyoming'}
  - dframe['state']=dfame['city'].map(state_map)
    - state_mapをcityをキーに追加
    - dframe['key1']=[0,1,2]で追加できるけど、ある列の値をkeyについかできる
- 37:置換
  - ser1 = Series([1,2,3,4,1,2,3,4])
  - ser1.replace(置き換えるもの,置き換え先のもの)
    - リストで渡せ、いっぺんに置換も
    - ディクショナリ型で{置き換えるもの:置き換え先のもの}でも渡せる
- 38:DataFrameのindexの変更
  - dframe = DataFrame(np.arange(12).reshape((3,4)),index=['NY','LA','SF'],columns=['A','B','C','D'])
  - dframe.index.map(str.lower)
    - str.lowerは小文字になる
    - dframe.index = dframe.index.map(str.lower)
      - これで変更される
  - dframe.rename(index=str.title,columns=str.lower)
    - str.title()は文章の先頭1文字が大文字になる関数)
    - 関数ポインタ？みたいのを渡す
  - dframe.rename(index={'ny':'NEW YORK'}, columns={'A':'ALPHA'})
    - 辞書を引数に、該当するものを変更する
    - inplace=Trueを渡すを破壊的に変更される
- 39:ビニング：データの分類
  - import pandas as pd
  - years = [1900,1991,1992,2008,2012,2015,1987,1969,2013,2008,1999]
  - decate_bins = [1960,1970,1980,1990,2000,2010,2020]
    - 10年ごとに集計してみるための指標
  - decate_cat = pd.cut(years,decate_bins)
    - (はふくまず、]は含む
  - decate_cat.categories
  - pd.value_counts(decate_cat)
    - それぞれのカテゴリにデータが何個あるか
  - pd.cut(years,2)
    - 全体が2グループに分けられる
  - でも大体ヒストグラムの機能にこれらgあ含まれるので...使わないかもしれない
- 40:外れ値
  - import numpy as np
  - import pandas import DataFrame
  - np.random.seed(12345)
    - 引数を同じ値を与ええらば、同じ乱数が得られる
  - dframe = DataFrame(np.random.randn(1000,4))
  - col = dframe[0]
    - col[np.abs(col)>3]
      - 絶対値が3以上のものだけを取り出す
  - np.abs(dframe)>3
    - この結果条件に合うかのTrue/Falseが返却
  - dframe[(np.abs(dframe)>3).any(1)]
    - 1はaxisの方向(列)
    - anyはどれかの列にTrueがあるか
      - →どこかの列に3より大きいものがある
  - np.sign(dframe)
    - それぞれの符号(-1/1)が返ってくる
  - dframe[np.abs(dframe)>3] = np.sign(dframe)*3
    - その場所がマイナスならば-3、プラスなら+3される
- 41:Permutation：ランダムに順列をバラバラにする
  - import numpy as np
  - import pandas import DataFrame
  - dframe = DataFrame(np.arange(4*4).reshape((4,4)))
  - blender = np.array([0,3,2,1])
  - dframe.take(blender)
    - 0行目は変わらない
    - 1行目が3行目となる
    - 次は2行目
    - 最後は1行目
  - blender = np.random.permutation(4)
    - array([2,3,1,0])みたいなものがランダムにもらえるので、これをdframe.take(blender)でランダムな行列になる
  - いままではあったデータをそうさしていたが、こんどはデータを取り出してはもとに戻す
  - box = np.array(['A','B','C'])
  - shaker = np.random.randint(0,len(box),size=10)
    - ０から2までの値で10このarray
  - hand_grabs = box.take(shaker)
    - A,B,Cのなかから、取り出した結果の配列のイメージ
  - シミュレーションのデータイメージ
- 42:DataFrameのGroupBy
  - import numpy as np
  - import pandas as pd
  - from pandas import DataFrame
  - dframe = DataFrame({'k1':['X','X','Y','Y','Z'],'k2':['alpha','beta','alpha','beta','alpha'],'dataset1':np.random.randn(5),'dataset2':np.random.randn(5)})
  - group1 = dframe['dataset1'].groupby(dframe['k1'])
    - dataset1の列についてk1の列についてまとめてみる
  - cities = np.array(['NY','LA','LA','NY','NY'])
  - month = np.array(['JAN','FEB','JAN','FEB','JAN'])
  - dframe['dataset1'].groupby([cities,month])
    - dataset1に対してもともとdataframeにはないものに対して処理できる
    - この場合同じものは、0番めのNY,JANと４番目のNY,JANなので、indexがそのものが出てくる
  - dframe.groupby(['k1','k2'])
    - 複数の列でまとめる場合リストを渡せばできる
  - dataset2_group = dframe.groupby(['k1','k2'])[ ['dataset2'] ]
    - 列を限定するdataset2だけとなる
  - dframe.groupby(['k1']).size()
    - それぞれのグループに何個あるか？
    - for name,group in dframe.groupby('k1'):
      - ...繰り返しで取得
  - gr = dframe.groupby('k1')
  - gr.get_group('X')
    - XのDataFrameを取得
  - いままでは行方向でgroupbyしたが列にもできる
    - 複雑でわからないのでメモしない
- 43:GroupByその2
  - import numpy as np
  - import pandas as pd
  - from pandas import Series, DataFrame
  - animals = DataFrame(np.arange(16).reshape(4,4),columns=['W','X','Y','Z'],index=['Dog','Cat','Bird','Mouse'])
  - animals.is[1:2,['W','Y']] = np.nan
    - 1:2 つまり1行目のW,Yをnanに
  - behavior_map = {'W':'bad','X':'good','Y':'bad','Z':'good'}
  - animals_col = animals.groupby(behavior_map, axis=1)
    - WとYはbadなのでひとまとまり
    - XとZはひとまとまり
  - ↑これをDictonaryではなくSeriesでもできる
  - behavior_series = Series(behavior_map)
  - animals.groupby(behavior_series, axis=1)
    - 同じことができた
  - groupbyには関数をあたえられる
  - animals.groupby(len)
    - lenはindexの文字列の長さでgroupbyされる
  - animals.groupby([len,['A','B','A','B']])
    - A,B,A,Bのところがよくわからんけどこんなことができるらしい
- 44:データのAggregation:たくさんあるデータから特徴的なデータ(max,mean,minなど)を抽出する
  - import numpy as np
  - import pandas as pd
  - from pandas import Series, DataFrame
  - url='http://archive.ics.uci.edu/ml/machine-lerning-database/wine-quality'
    - ワインの質に関係するサンプルデータ
    - セミコロンで区切られている
    - 1行が一本のワインを表している
  - dframe_wine = pd.read_csv(ファイル名,sep=';')
  - def max_to_min(arr):
    - return arr.max() - arr.min()
  - wino = dframe_wine.groupby('quality')
  - wino.agg(max_to_min)
    - qualityでグルーピングした各列に対してmax_to_minを計算する
  - wino.agg('mean')
    - 文字列も渡せてこの場合平均値を計算してくれる
  - dframe_wine['qual/alc raito'] = dframe_wine['quality'] /dframe_wine['alcohol']
    - ↑新しい列を追加することは簡単
  - dframe_wine.pivot_table(index=['quality'])
    - groupbyしたときと同じ値をとれる
  - 可視化の件
  - dframe_wine.plot(kind='scatter', x='quality',y='alcohol')
    - 散布図
- 45:Split,Apply,Combine
  - Split:分割group by
  - Apply:分割された毎に何か計算(平均値とか)
  - Combine:連結して結果を表示
  - import numpy as np
  - import pandas as pd
  - from pandas import Series, DataFrame
  - 44のワインのデータをつかう
  - dframe_wine = pd.read_csv(ファイル名,sep=';')
  - qualityごとにアルコール度数が高いものを出す
  - def ranker(df): #アルコール度数のランク付け
    - df['alc_content_rank'] = np.arange(len(df)) + 1
    - return df
  - dframe_wine.sort('alcohol',ascending=False,inplace=True)
    - ascending=Falseは降順
    - 破壊的にソート:inplace=True
  - dframe_wine = dframe_wine.groupby('quality').apply(ranker)
  - num_of_qual = dframe_wine['quality'].value_counts()
    - それぞれのデータが何個あるか？
  - dframe_wine[dframe_wine.alc_count_rank==1]
- 46:クロス集計
  - import pandas as pd
  - import io import StringIO #文字列をファイルのように読み書きする
  - data = '''Sample Animal Intelligense
    1 Dog   Dumb
    2 Dog Dumb
    3 Cat       Smart
    4 Cat    Smart
    5 Dog Smart
    6 Cat  Smart'''
    - スペースは適当で構わない
  - dframe = pd.read_tables(StringIO(data), sep='\s+')
    - \s+は正規表現で空白1回以上の繰り返し
  - pd.crosstab(dframe.Animal,dframe.Intelligense)
    - クロス集計表
  - pd.crosstab(dframe.Animal,dframe.Intelligense,margins=Ture)
    - 行と列ごとに合計を追加してくれる
- 47:Seaborn
  - 非常に優れたデータ可視化ライブラリ
    - 色を簡単に変えられる
- 48:ヒストグラム
  - from numpy.randm import randn
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - %matplotlib inline(jupyter用)
  - dataset1 = randn(100)
    - 正規分布
  - plt.hist(dataset1)
    - デフォルト10区切り
  - dataset2 = randn(80)
  - plt.hist(dataset2, color='indianred')
    - 色を変えた
  - plt.hist(dataset1, normed=True)
    - 面積をすべて足すを１になるようにする（標準化：形式をあわせられる）
    - 何がいいかというと、重ねられる↓
  - plt.hist(dataset1, normed=True, alpha=0.5, bins=20)
  - plt.hist(dataset2, normed=True, alpha=0.5, bins=20, color='indianred')
  - 上記を同時実行する(jpyterで)
    - 重なって表示される
  - data1 = randn(1000)
  - data2 = randn(1000)
  - sns.jointplot(data,1,data2) #結合分布,結合分布というもの
    - sns.jointplot(data,1,data2,kind='hex')
      - pointではなく、色の濃さの６角形で表現される
  - 未知のデータを見た時にヒストグラムで表現するのが初めの手段
- 49:カーネル密度推定
  - 超簡単に言うと、なめらかなヒストグラムを作る
  - 別の細かい関数の足し合わせでなめらかな曲線を描く
  - seabornを使うと超絶簡単に
  - import numpy as np
  - from numpy.random import randn
  - from scipy import stats
  - import mathplotlib as mpl
  - import mathplotlib.pyplot as plt
  - import seaborn as sns
  - %matplotlib inlne
  - dataset = randn(25)
  - sns.rugplot(dataset)
    - データがあるところに線が
  - plt.hist(dataset,alpha=0.3)
  - sns.rugplot(ataset)
    - 重ねてみるといい感じになっているかと
  - BandWidthSelection
    - wikipediaの値を採用してみる
  - sns.rugplot(dataset)
  - x_min = dataset.min() - 2
  - x_man = dataset.max() + 2
  - x_axis = np.linspace(x_min,x_max),100)
  - ↑最大値と最小値を100等分した軸を作る
  - bandwidth = ((4*datasset.std()**5)/(3*len(dataset)))**0.2
    - ↑wikipediaの式をそのまま
  - kernel_list = []
  - for data_point in dataset: #ポイント事にkernelを作成
    - kernel = stats.norm(data_point, bandwidth).pdf(x_axis)
    - kernel_list.append(kernel)
    - kernel = kernel / kernel.max()
    - kernel = kernel * 0.4
    - plt.plot.(x_axis, kernel ,color = 'gray' , alpha=0.5)
  - plt.ylim(0,1)
  - sum_of_kde = np.sum(kernel_list,axis=0)　#カーネルの足し合わせる
  - fig = plt.plot(x_axis,sum_of_kde ,color='indianred)
  - sns.rugplot(dataset)
  - plt.yticks([]) #y軸に空をわたして消している
  - plt.suptitle('Sum of the Basis Functions') #日本語を書くと文字化ける。回避方法は資料に
  - ↑これをseabornをつかうと１行でできる！↓
  - sns.kdeplot(dataset)
  - 説明
  - sns.rugplot(dataset,color='black')
  - for bw in np.arange(0.5,2,0.25)
    - sns.kdeplot(dataset,bw=bw,label=bw)
  - ↑bandwidthの幅によってどのような影響があるか
    - 狭いとぐぐぐ！と高くなり、広いと裾が広がる
  - ↑はガウス分布を使ったが、他にどのような物があるか
  - kernel_options = ['biw','cos','gau','tri','triw']
  - for kern in kernel_options:
    - sns.kdeplot(dataset,kernel=kern,label=kern)
  - ↑いろいろな規定関数の差がわかる
  - 累積分布関数
  - plt.hist(dataset,cumulative=True)
  - ↑これもがたがたになるので、
    - sns.kdeplot(dataset,cumulative=True)
    - cumulative積み上げいく
  - ２次元でも出来る
  - mean=[0,0] #原点を平均
  - cov=[ [1,0], [0,100] ]　#それぞれの方向への分散
  - dataset2 = np.random.multivariate_nomal(mean,cov,1000) #ランダムに1000点
  - dframe = pd.DataFrame(dataset2,columns=['X','Y'])
  - sns.kdeplot(dframe)
    - 軸ごとにデータもわたせて
    - sns.kdeplot(dframe.X,dframe.Y,shade=True)
  - バンド幅も
    - sns.kdeplot(dframe,bw=1)
  - 他の推定方法も
    - sns.kdeplot(dframe,bw='silverman')
      - くわしくはドキュメントを
  - sns.jointplot('X','Y',dframe,kind='kde')
- 50:分布の可視化
  - from numpy.random import randn
  - import seaborn as sns
  - %matplotlib inline
  - dataset = randn(100)
  - sns.distplot(dataset)
    - カーネル密度推定と、ヒスとグラムも
  - sns.distplot(dataset,rug=True,hist=False)
    - 表示するグラフを選択出来る
  - sns.distplot(dataset,bins=25,kde_kws={'color:indeanred,'label':'KDE PLOT'})
    - カーネル密度推定の色を変える
  - Seriesと親和性が高いPandasと
  - from pandas import Series
  - ser1 = Series(dataset,name='My_DATA',)
  - sns.distplot(ser1)
- 51:box port,ヴァイオリンプロット
  - import numpy as np
  - from numpy.random import randn
  - from scipy import stats
  - import seaborn as sns
  - %matplotlib inline
  - data1 = randn(100)
  - data2 = randn(100)+2
  - sns.boxplot(data=[data1,data2])
    - はこひげ図
    - 真ん中が中央値
    - 箱は25~75%の所
    - 外れ値がある（あまりにも大きい・小さい値）
    - 外れ値を含む長いひげを描く場合
      - sns.boxplot(data=[data1,data2],whis=np.inf)
    - 横に描画
      - sns.boxplot(data=[data1,data2],orient='h')
      - 縦はorient='v'
  - バイオリンプロット
  - data1 = stats.norm(0,5)rvn(100)
    - 平均が０標準偏差が5の100
  - data2 = np.concatenate([stats.gamma(5).rvs(50) -1 ,-1 *stats.gamma(5).rvs(50)])
  - sns.violinplot(data=[data1,data2])
    - sns.boxplot(data=[data1,data2])
    - と形は同じだが、violinplotを描いてみると全然違う
  - カーネル密度推定と同じような書き方なのでバンド幅を変えられる
    - sns.violinplot(data=[data1,data2],bw=0.01)
  - sns.violinplot(data=data1,inner='stick')
    - バイオリンプロットのなかでデータがどこにあるか示せる
- 52:回帰とプロット
  - import numpy as np
  - from numpy.random import randn
  - from scipy import stats
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - %matplotlib inline
  - seaboanにはサンプルデータも含まれている
  - tips = sns.load_dataset('tips')
    - チップのデータ
  - sns.lmplot('total_bill','tip',tips)
    - 回帰直線を書く
      - tipsの名前のDataFrameから
      - X:totalbill
      - Y:tip
    - 薄く色がついているところは信頼区間と呼ばれている
  - sns.lmplot('total_bill','tip',tips,scatter_kws={'marker':'o','color':'indianred'},line_kws={'linewidth':1,'color':'blue'})
    - 色付け
  - sns.lmplot('total_bill','tip',tips,order=4,scatter_kws={'marker':'o','color':'indianred'},line_kws={'linewidth':1,'color':'blue'})
    - order=*で高次元の多項式でフィットさせる
    - おおお！
  - sns.lmplot('total_bill','tip',tips,fit_reg=False)
    - 回帰線を描かない：fit_reg
  - tips['tip_pect'] = 100*(tips['tip']/tips['total_bill'])
    - 新しい列を追加
      - 総額のいくら払ったかの列
  - sns.lmplot('size','tip_pect',tips)
    - sizeは連続じゃなく、離散的な値
    - それもうまく簡単にかける
  - sns.lmplot('size','tip_pect',tips,x_jitter=0.2)
    - jitter揺らぎ？？
    - ばらける
  - sns.lmplot('size','tip_pect',tips,x_estimator=np.mean)
    - チップのパーセントの平均値
      - 2,3,4人の払う額は安定している：グラフから
  - sns.lmplot('total_bill','tip_pect',hue='sex',markers=['x','o']
    - hueで指定した列をmarkersでかく
  - sns.lmplot('total_bill','tip_pect',tips,lowess=True)
    - lowess
    - 回帰直線が少しなめらかにするととう
    - 局所的にデータを見て線を引く
  - sns.regplot('total_bill','tip_rect',tips)
    - lmportはregplotを呼んでいる
  - seaboan自体はmatprotlibを呼んでいる
  - 二つのグラフを並べて書く
    - fig,(axis1,axis2) = plt.subplots(1,2,sharey=True)
      - 1行2列の描画領域を取得(axis1,axis2)
      - shareyでY軸を共有する
    - sns.regplot('total_bill','tip_pect',tips,ax=axis1)
    - sns.violinplot(y='tip_pect' x='size',data=tips.sort('size'),ax=axis2)
    - matplotlibとseaboanは親和性が高い
- 53:ヒートマップとクラスタリング
  - import numpy as np
  - from numpy.random import randn
  - from scipy import stats
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - %matplotlib inline
  - またサンプルを使う
  - flight_dframe = sns.load_dataset('flights')
  - flight_dframe = flight_dframe.pivod('month','year','passengers')
    - データ形式変更
  - sns.heatmap(flight_dframe)
    - おおおお！
  - sns.heatmap(flight_dframe,annot=True,fmt='d')
    - ヒートマップに数字が上書き！
  - sns.heatmap(flight_dframe,center=flight_dframe.loc['January',1955])
    - 1955年1月を基準に色を変えてみる
  - 二つのグラフを書いてみる
    - f,(axis1,axis2) = plt.subplots(2,1)
    - yearly_flights = flight_dframe.sum()
      - 12か月ごとの合計値
    - years = ps.Series(yearly_flights.index.values)
    - years = pd.DataFrame(years)
    - flights = pd.Series(yearly_flights.values)
    - flights = pd.DataFrame(flights)
    - year_dframe = pd.concat((years,fights),axis=1)
    - year_dframe.columns = ['Year','Flights']
    - 
    - sns.barplot('Year', y='Flights',data=year_dframe,ax=axis1)
    - sns.heatmap(flight_dframe,cmap='Blues',ax=axis2,cbar_kws={'orientation':'horizontal'})
  - クラスタ化したヒートマップ
    - sns.clustermap(flight_dframe)
      - 行ごと、列ごとに近いものが集まる
  - sns.clustermap(flight_dframe,col_cluster=False)
    - 列方向のクラスタリングをしない
  - データの標準化
    - sns.clustermap(flight_dframe,standard_scale=1)
      - 1は列方向
      - 0は行方向
  - zスコア：例のzスコア
    - sns.clustermap(flight_dframe,z_score=1)
- 56～:Titanic号の沈没解析
  - import numpy as np
  - import matplotlib.pyplot as plt
  - import seaborn as sns
  - import pandas as pd
  - from pandas import Series, DataFrame
  - $matplotlib inline
  - まず性別
    - sns.countplot('Sex',data=titanic_df)
      - 男性が多い
    - sns.countplot('Sex',data=titanic_df,hue='Pclass')
      - 客室ランクでわけ
    - sns.countplot('Pclass',data=titanic_df,hue='Sex')
      - 客室ランクで性別
  - 子供のデータを入れる。年齢から分ける
    - def male_female_child(passenger):
      - age, sex = passenger
      - if age < 16:
        - return 'child' #16歳未満子供ならchild
      - else:
        - reurn sex #大人であればどちらかの性別
    - 新しい列を作る
    - titanic_df['person'] = titanic_df[ ['Age','Sex] ].apply(male_female_child, axis=1)
      - 年齢と性別の列をとってきて関数に渡し、personの列ができる
    - sns.countplot('Pclass',data=titanic_df,hue='pserson')
      - 大人の男女・こどもが
  - titanic_df['Age'].hist(bins=70)
    - 年齢別ヒストグラム描画
    - titanic_df['Age'].mean() #平均年齢
    - titanic_df['person'].value_counts()
      - 男女・子供のそれぞれのカウント
  - カーネル密度推定で描いてみる
    - fig = sns.FacetGrid(titanic_df, hue='person', aspect=4)
      - aspect:表の幅
    - fig.map(sns.kdeplot, 'age', shade=True)
    - oldest = titanic_df['Age'].max()
    - fig.set(xlim=(0,oldest))
      - 0歳からmax年齢まで
    - figadd_legend()
    - 結果、客室等級がさがるにわかい
  - どこにいたか
    - deck = titanic_df['Cabin'].dropna()
      - A～Gみたいなものあどの階にいたか
    - levels = []
    - for level in deck:
      - levels.append(level[0])
    - cabin_df = DataFrame(levels)
    - cabin_df.columns = ['Cabin']
    - ↑これを使ってどこにどれぐらいの人がいたか
    - sns.countplot('Cabin', data=cabin_df, palette='winter_d', order=sorted(set(levels)))
      - levelsをSetオブジェクトにして並び替えて置く
    - cabin_df = cabin_df[cabin_df.Cabin != 'T']
      - T階？は誤りだと思われるので削除する
  - どこから乗ってきたか
    - Embarked:港
    - 乗った港と客室ランクの関係
      - sns.countplot('Embarked', data=titanic_df, hue='Pclass')
  - from collections import Counter
  - Counter(titanic_df.Embarked)
    - 港のカウント
    - 実行してみると判るがnanがある
      - seaboanなどのフレームワークはnanを意識させないようになっている
    - 時々nanを意識しないと合計などがおかしくなると思う
