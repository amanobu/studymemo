- install
  - CPU版
    - pip install tensorflow
  - GPU版
    - pip install tensorflow-gpu
- 最初のプログラム
  - import tensorflow as tf
  - hello = tf.constant('Hello.TensorFlow')
  - sess = tf.Session()
  - print(sess.run(hello))
- 処理の流れ
  - TensorFlowの読み込み
  - 式を立てる：モデルを構築？
  - セッション開始
  - 実行
- 多次元行列：Tensor
- MINIST Beginnersをやる
  - SoftMAX
    - すべて足すと１になるように計算する関数
    - こんな関数をTensorFlowはたくさん定義されている
  - クロスエントロピー
    - 正解との差を表現するために
  - y  = softmax(W・x + b)
    - W:重み
    - x:濃淡情報
    - b:バイアス
    - y:答え
  - グラフ：Tensorのデータの流れの事を指している
    - x
    - w
    - →MatMul(W・x)
    - b
    - →Matmul(W・x) + b
    - →SoftMAX
    - ↑こんな感じで流れるから
  - コード
    - import tensorflow as tf
    - x = tf.placeholder(tf.float32,[None,784])
      - 0/1で32桁、初期値なしで、784個のデータの箱を用意する
    - W = tf.Variable(tf.zeros([784,10])) #Variable:変数,初期値0
    - b = tf.Variable(tf.zeros([10]))
    - y = tf.nn.softmax(tf.matmul(x,W) + b) #matmul:行のかけ算をするという指定
    - y_ = tf.placeholder(tf.float32,[None,10]) #正解を格納する変数 行数が不明で10桁
    - cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
      - 類似度の計算：推定値とラベルデータのずれを計算する
        - reduce_mean:平均
        - reduce_sum:総和
    - train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
      - 勾配降下法でcross_entropyを最小するようにしやがれ
      - 学習率0.5で
    - init = tf.global_variables_initializer()
    - sess = tf.Session()
    - sess.run(init)
    - for i in range(1000): #ミニバッチ
      - batch_xs,batch_ys = mnist.train.next_batch(100) #ランダム100個で処理せよ
      - sess.run(train_step, feed_dict = {x: batch_xs, y_: batch_ys}) #学習
    - correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))
      - argmax:行列の中で一番大きい物を取り出す
        - 取り出して、比較
    - accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
      - reduce_mean:精度の平均値。
      - cast:True,Falseを数値(割合)に変換
    - print(sess.run(accuracy, feed_dict = {x: mnist.test.images, y_:mnist.test.labels}))
      - 正解率の取り出し
      - このmovieでは99.7まであがっているらしい
- 今度はMINIST Expertをやる
  - 28*28のinput画像を最終的に7*7picに変換し、先ほどのSOFTMAX関数で判定を行う
    - 28*28の32枚→5*5の32枚のフィルタ→14*14を32枚
    - 13*13の64枚→5*5の32枚のフィルタ→7*7の64枚
    - これを全結合してsoftmaxで確率化
  - 
- 畳み込み処理
  - 画像に対して、Filter(今回の場合5*5とか)のフィルタをかける
  - それを少しずつずらして処理をする：スライド幅・ストライド
    - 端に来たら次の行
  - それを一つ一つ、足し算し、新しい値を得る
    - 元々の画像をこの変換する事を畳み込むという
      - convolve
- パディング
  - 28*28の画像を5*5でフィルタすると24*24になる。
  - それだと画像サイズが小さくなｒうまくいかないので、周りに0で埋めるという事を実施する
- プーリング処理
  - 画像の一がずれていてもいい感じに動くようにする
    - つまり、特徴量の抽出がうまくいくようにフィルタをかける
  - やり方が色々
    - MAXプーリング
      - 今回採用のした物
      - 選択した部分の最大値
    - 平均プーリング
      - 平均値をとる
- 活性化関数
  - シグモイド(0 ～ 1)
  - tanh(-1 ～ 1)
  - ReLU(z<0 → y=0, z>0→y = z)
    - 速度が速い
    - 画像認識部分でよく使われるとの事
    - 今回はこれを使っている
  - LeakyReULU(z<0 → y=az, z>0→y=z)
    - ReLUと違いはy=0に張り付かず、多少の傾きがある
  - SOFTMAX
    - すべての出力値を足すと1になるように変換をする
      - 入力値を確率に変換している
